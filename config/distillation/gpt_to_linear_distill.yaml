# GPT到Linear知识蒸馏配置

# 蒸馏配置
distillation:
  alpha: 0.5               # ground truth损失权重
  beta: 0.5                # 预测蒸馏损失权重
  temperature: 4.0         # 软化温度
  use_feature_distill: true # 是否使用特征蒸馏
  feature_loss_weight: 1.0 # 特征蒸馏损失权重

# 训练配置
train_config:
  epochs: 100
  batch_size: 32
  lr_max: 0.001
  weight_decay: 0.01
  optimizer: Adam
  scheduler: cosine
  save_freq: 10
  num_workers: 4 