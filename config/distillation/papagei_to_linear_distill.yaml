distillation:
  # 蒸馏损失权重
  alpha: 0.2      # ground truth损失权重
  beta: 0.8       # 预测蒸馏损失权重
  temperature: 4.0  # 软化温度
  
  # 特征蒸馏设置
  use_feature_distill: true
  feature_loss_weight: 1.5  # 对Linear模型使用更高的特征蒸馏权重

train_config:
  epochs: 120
  batch_size: 32
  lr_max: 0.0005  # 较低的学习率，因为Linear模型较简单
  optimizer: "AdamW"
  weight_decay: 0.01
  scheduler: "cosine"
  
  # 保存设置
  save_freq: 10
  
  # 早停参数
  patience: 20
  min_delta: 0.001 