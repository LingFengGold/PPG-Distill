#!/usr/bin/env bash
#SBATCH --job-name=ppg_ablation
#SBATCH --partition=i.q
#SBATCH --mem=65000
#SBATCH --gpus=1
#SBATCH --time=12:00:00
#SBATCH --output=log/ablation_%A_%a.out
#SBATCH --error=log/ablation_%A_%a.err
#SBATCH --array=0-5

source ~/anaconda3/etc/profile.d/conda.sh
conda activate py312

# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p log
mkdir -p output_ablation

# ============================================================================
# æ¶ˆèå®éªŒé…ç½®åŒºåŸŸ
# ============================================================================

# åŸºç¡€å‚æ•°è®¾ç½®
DATASET="dalia"
TEACHER_TYPE="gpt_19m"
STUDENT_TYPE="gpt_1m"
SAVE_DIR="./output"
SAVE_DIR_STUDENT="./output_ablation"
SEED=42

# æ¶ˆèå®éªŒé…ç½®åˆ—è¡¨
ABLATION_CONFIGS=(
    "full_distill:å®Œæ•´è’¸é¦(baseline)"
    "no_feature_distill:å»é™¤Feature-levelè’¸é¦"
    "no_label_distill:å»é™¤Label-levelè’¸é¦"
    "no_patch_contrastive:å»é™¤Patch_Contrastive_Level"
    "no_patch_relational:å»é™¤Patch_Relational_Level"
    "no_ground_truth:å»é™¤Ground_Truth_Loss"
)

TOTAL_EXPERIMENTS=${#ABLATION_CONFIGS[@]}
MAX_ARRAY_INDEX=$((TOTAL_EXPERIMENTS - 1))

echo "=== PPG-GPT çŸ¥è¯†è’¸é¦æ¶ˆèå®éªŒ ==="
echo "æ•°æ®é›†: $DATASET"
echo "Teacheræ¨¡å‹: $TEACHER_TYPE"
echo "Studentæ¨¡å‹: $STUDENT_TYPE"
echo "æ€»å®éªŒæ•°: $TOTAL_EXPERIMENTS"
echo "æ•°ç»„ç´¢å¼•èŒƒå›´: 0-$MAX_ARRAY_INDEX"
echo ""

# æ£€æŸ¥SLURMæ•°ç»„ä»»åŠ¡IDæ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
if [ -n "$SLURM_ARRAY_TASK_ID" ]; then
    if [ $SLURM_ARRAY_TASK_ID -gt $MAX_ARRAY_INDEX ]; then
        echo "âŒ é”™è¯¯: SLURM_ARRAY_TASK_ID ($SLURM_ARRAY_TASK_ID) è¶…å‡ºæœ‰æ•ˆèŒƒå›´ (0-$MAX_ARRAY_INDEX)"
        echo "è¯·æ›´æ–°SBATCH --arrayå‚æ•°ä¸º: --array=0-$MAX_ARRAY_INDEX"
        exit 1
    fi
    exp_id=$SLURM_ARRAY_TASK_ID
else
    # å¦‚æœä¸åœ¨SLURMç¯å¢ƒä¸­ï¼Œä½¿ç”¨é»˜è®¤å€¼è¿›è¡Œæµ‹è¯•
    exp_id=0
    echo "âš ï¸  è­¦å‘Š: ä¸åœ¨SLURMç¯å¢ƒä¸­ï¼Œä½¿ç”¨é»˜è®¤å®éªŒID: $exp_id"
fi

# è·å–å½“å‰å®éªŒé…ç½®
current_config=${ABLATION_CONFIGS[$exp_id]}
config_name=${current_config%%:*}
exp_description=${current_config##*:}

echo "=== å½“å‰å®éªŒé…ç½® ==="
echo "å®éªŒ ID: $exp_id"
echo "é…ç½®åç§°: $config_name"
echo "å®éªŒæè¿°: $exp_description"
echo "å¼€å§‹æ—¶é—´: $(date)"
echo ""

# ============================================================================
# æ£€æŸ¥Teacheræ¨¡å‹
# ============================================================================

teacher_model_path="$SAVE_DIR/${TEACHER_TYPE}_${DATASET}_best.pth"
if [ ! -f "$teacher_model_path" ]; then
    echo "âŒ Teacheræ¨¡å‹ä¸å­˜åœ¨: $teacher_model_path"
    echo "è¯·å…ˆè®­ç»ƒteacheræ¨¡å‹æˆ–æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®"
    exit 1
fi

echo "âœ… æ‰¾åˆ°Teacheræ¨¡å‹: $teacher_model_path"
teacher_size=$(du -h "$teacher_model_path" | cut -f1)
echo "Teacheræ¨¡å‹å¤§å°: $teacher_size"

# ============================================================================
# åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
# ============================================================================

# åˆ›å»ºä¸´æ—¶é…ç½®ç›®å½•
TEMP_CONFIG_DIR="./config/ablation_temp_${SLURM_JOB_ID}_${exp_id}"
mkdir -p $TEMP_CONFIG_DIR

echo "åˆ›å»ºæ¶ˆèå®éªŒé…ç½®æ–‡ä»¶: $config_name"

# æ ¹æ®å®éªŒIDåˆ›å»ºå¯¹åº”çš„é…ç½®æ–‡ä»¶
case $exp_id in
    0)  # å®Œæ•´è’¸é¦ (baseline)
        cat > $TEMP_CONFIG_DIR/${config_name}.yaml << 'EOF'
distillation:
  alpha: 0.1      # é¢„æµ‹è’¸é¦æŸå¤±æƒé‡
  beta: 0.1       # å…¨å±€ç‰¹å¾è’¸é¦æŸå¤±æƒé‡
  gamma: 0.5      # patchçº§åˆ«è’¸é¦æŸå¤±æƒé‡
  temperature: 2.0
  
  use_feature_distill: true
  feature_loss_weight: 1.0
  
  use_patch_feature_distill: true
  patch_feature_loss_weight: 1.0
  patch_distill_mode: 'contrastive'
  contrastive_temperature: auto
  
  use_patch_distance_distill: true
  patch_distance_loss_weight: 1.0
  
  tie_feature_and_patch: true

train_config:
  epochs: 120
  batch_size: 64
  lr_init: 1e-5
  lr_max: 0.001
  lr_final: 1e-6
  lr_schedule_ratio: 1
  lr_warm_up: 0.25
  optimizer: "AdamW"
  weight_decay: 0.1
  
  save_freq: 10
  early_stop_patience: 20
  early_stop_min_delta: 1e-6
EOF
        ;;
    1)  # å»é™¤ Feature-level è’¸é¦
        cat > $TEMP_CONFIG_DIR/${config_name}.yaml << 'EOF'
distillation:
  alpha: 0.1      # é¢„æµ‹è’¸é¦æŸå¤±æƒé‡
  beta: 0.0       # å…¨å±€ç‰¹å¾è’¸é¦æŸå¤±æƒé‡ (è®¾ä¸º0)
  gamma: 0.5      # patchçº§åˆ«è’¸é¦æŸå¤±æƒé‡
  temperature: 2.0
  
  use_feature_distill: false
  feature_loss_weight: 0.0
  
  use_patch_feature_distill: true
  patch_feature_loss_weight: 1.0
  patch_distill_mode: 'contrastive'
  contrastive_temperature: auto
  
  use_patch_distance_distill: true
  patch_distance_loss_weight: 1.0
  
  tie_feature_and_patch: true

train_config:
  epochs: 120
  batch_size: 64
  lr_init: 1e-5
  lr_max: 0.001
  lr_final: 1e-6
  lr_schedule_ratio: 1
  lr_warm_up: 0.25
  optimizer: "AdamW"
  weight_decay: 0.1
  
  save_freq: 10
  early_stop_patience: 20
  early_stop_min_delta: 1e-6
EOF
        ;;
    2)  # å»é™¤ Label-level (é¢„æµ‹è’¸é¦)
        cat > $TEMP_CONFIG_DIR/${config_name}.yaml << 'EOF'
distillation:
  alpha: 0.0      # é¢„æµ‹è’¸é¦æŸå¤±æƒé‡ (è®¾ä¸º0)
  beta: 0.1       # å…¨å±€ç‰¹å¾è’¸é¦æŸå¤±æƒé‡
  gamma: 0.5      # patchçº§åˆ«è’¸é¦æŸå¤±æƒé‡
  temperature: 2.0
  
  use_feature_distill: true
  feature_loss_weight: 1.0
  
  use_patch_feature_distill: true
  patch_feature_loss_weight: 1.0
  patch_distill_mode: 'contrastive'
  contrastive_temperature: auto
  
  use_patch_distance_distill: true
  patch_distance_loss_weight: 1.0
  
  tie_feature_and_patch: true

train_config:
  epochs: 120
  batch_size: 64
  lr_init: 1e-5
  lr_max: 0.001
  lr_final: 1e-6
  lr_schedule_ratio: 1
  lr_warm_up: 0.25
  optimizer: "AdamW"
  weight_decay: 0.1
  
  save_freq: 10
  early_stop_patience: 20
  early_stop_min_delta: 1e-6
EOF
        ;;
    3)  # å»é™¤ Patch Contrastive Level
        cat > $TEMP_CONFIG_DIR/${config_name}.yaml << 'EOF'
distillation:
  alpha: 0.1      # é¢„æµ‹è’¸é¦æŸå¤±æƒé‡
  beta: 0.1       # å…¨å±€ç‰¹å¾è’¸é¦æŸå¤±æƒé‡
  gamma: 0.5      # patchçº§åˆ«è’¸é¦æŸå¤±æƒé‡
  temperature: 2.0
  
  use_feature_distill: true
  feature_loss_weight: 1.0
  
  # å®Œå…¨ç¦ç”¨patchç‰¹å¾è’¸é¦ï¼ˆå»é™¤contrastiveï¼‰
  use_patch_feature_distill: false
  patch_feature_loss_weight: 0.0
  patch_distill_mode: 'direct'
  contrastive_temperature: auto
  
  # ä¿æŒpatchè·ç¦»è’¸é¦
  use_patch_distance_distill: true
  patch_distance_loss_weight: 1.0
  
  tie_feature_and_patch: true

train_config:
  epochs: 120
  batch_size: 64
  lr_init: 1e-5
  lr_max: 0.001
  lr_final: 1e-6
  lr_schedule_ratio: 1
  lr_warm_up: 0.25
  optimizer: "AdamW"
  weight_decay: 0.1
  
  save_freq: 10
  early_stop_patience: 20
  early_stop_min_delta: 1e-6
EOF
        ;;
    4)  # å»é™¤ Patch Relational Level (è·ç¦»è’¸é¦)
        cat > $TEMP_CONFIG_DIR/${config_name}.yaml << 'EOF'
distillation:
  alpha: 0.1      # é¢„æµ‹è’¸é¦æŸå¤±æƒé‡
  beta: 0.1       # å…¨å±€ç‰¹å¾è’¸é¦æŸå¤±æƒé‡
  gamma: 0.5      # patchçº§åˆ«è’¸é¦æŸå¤±æƒé‡
  temperature: 2.0
  
  use_feature_distill: true
  feature_loss_weight: 1.0
  
  # ä¿æŒpatchç‰¹å¾è’¸é¦
  use_patch_feature_distill: true
  patch_feature_loss_weight: 1.0
  patch_distill_mode: 'contrastive'
  contrastive_temperature: auto
  
  # ç¦ç”¨patchè·ç¦»è’¸é¦
  use_patch_distance_distill: false
  patch_distance_loss_weight: 0.0
  
  tie_feature_and_patch: true

train_config:
  epochs: 120
  batch_size: 64
  lr_init: 1e-5
  lr_max: 0.001
  lr_final: 1e-6
  lr_schedule_ratio: 1
  lr_warm_up: 0.25
  optimizer: "AdamW"
  weight_decay: 0.1
  
  save_freq: 10
  early_stop_patience: 20
  early_stop_min_delta: 1e-6
EOF
        ;;
    5)  # å»é™¤ Ground Truth Loss
        cat > $TEMP_CONFIG_DIR/${config_name}.yaml << 'EOF'
distillation:
  alpha: 0.8      # å¢åŠ é¢„æµ‹è’¸é¦æŸå¤±æƒé‡æ¥è¡¥å¿
  beta: 0.1       # å…¨å±€ç‰¹å¾è’¸é¦æŸå¤±æƒé‡
  gamma: 0.5      # patchçº§åˆ«è’¸é¦æŸå¤±æƒé‡
  temperature: 2.0
  
  use_feature_distill: true
  feature_loss_weight: 1.0
  
  use_patch_feature_distill: true
  patch_feature_loss_weight: 1.0
  patch_distill_mode: 'contrastive'
  contrastive_temperature: auto
  
  use_patch_distance_distill: true
  patch_distance_loss_weight: 1.0
  
  tie_feature_and_patch: true

train_config:
  epochs: 120
  batch_size: 64
  lr_init: 1e-5
  lr_max: 0.001
  lr_final: 1e-6
  lr_schedule_ratio: 1
  lr_warm_up: 0.25
  optimizer: "AdamW"
  weight_decay: 0.1
  
  save_freq: 10
  early_stop_patience: 20
  early_stop_min_delta: 1e-6
EOF
        ;;
    *)
        echo "âŒ é”™è¯¯: æ— æ•ˆçš„å®éªŒID: $exp_id"
        exit 1
        ;;
esac

echo "âœ… åˆ›å»ºé…ç½®æ–‡ä»¶å®Œæˆ: $TEMP_CONFIG_DIR/${config_name}.yaml"

# ============================================================================
# åˆ›å»ºæ¶ˆèå®éªŒè®­ç»ƒè„šæœ¬
# ============================================================================

# åˆ›å»ºä¿®æ”¹ç‰ˆçš„è®­ç»ƒè„šæœ¬æ¥æ”¯æŒè‡ªå®šä¹‰é…ç½®æ–‡ä»¶
cat > train_distill_ablation.py << 'EOF'
#!/usr/bin/env python
"""
æ¶ˆèå®éªŒç‰ˆæœ¬çš„çŸ¥è¯†è’¸é¦è®­ç»ƒè„šæœ¬
æ”¯æŒè‡ªå®šä¹‰é…ç½®æ–‡ä»¶è·¯å¾„å’Œå»é™¤ground truthæŸå¤±
"""

import os
import sys
sys.path.append('.')
import argparse
import torch
import logging
from logging import info as lprint
import yaml
import json

# å¯¼å…¥åŸå§‹è®­ç»ƒè„šæœ¬çš„æ‰€æœ‰ç»„ä»¶
from train_distill import *

def load_custom_config(distill_config_path, student_config_path, data_config_path):
    """åŠ è½½è‡ªå®šä¹‰é…ç½®æ–‡ä»¶"""
    distill_config = load_config(distill_config_path)
    student_model_config = load_config(student_config_path)
    data_config = load_config(data_config_path)
    
    # åˆå¹¶é…ç½®
    config = {
        'distillation': distill_config['distillation'],
        'train_config': distill_config['train_config'],
        'data_config': data_config['data_config']
    }
    
    # è·å–ä»»åŠ¡ç±»å‹
    task_type = data_config['data_config'].get('task_type', 'regression')
    
    return config, student_model_config['model_config'], task_type

class AblationDistillationLoss(DistillationLoss):
    """æ¶ˆèå®éªŒç‰ˆæœ¬çš„è’¸é¦æŸå¤±ï¼Œæ”¯æŒå®Œå…¨å»é™¤ground truthæŸå¤±"""
    
    def __init__(self, *args, use_ground_truth=True, **kwargs):
        super().__init__(*args, **kwargs)
        self.use_ground_truth = use_ground_truth
    
    def forward(self, student_output, teacher_output, student_features=None, 
                teacher_features=None, targets=None, task_type='regression',
                student_patch_features=None, teacher_patch_features=None):
        """
        è®¡ç®—è’¸é¦æŸå¤±ï¼ˆæ”¯æŒå»é™¤ground truthæŸå¤±çš„æ¶ˆèå®éªŒï¼‰
        """
        
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•è·å–æ‰€æœ‰æŸå¤±
        loss_dict = super().forward(
            student_output, teacher_output, student_features, 
            teacher_features, targets, task_type,
            student_patch_features, teacher_patch_features
        )
        
        # å¦‚æœç¦ç”¨ground truthæŸå¤±ï¼Œé‡æ–°è®¡ç®—æ€»æŸå¤±
        if not self.use_ground_truth:
            total = 0.0
            # åªåŒ…å«è’¸é¦æŸå¤±ï¼Œä¸åŒ…å«ground truthæŸå¤±
            total += self.alpha * loss_dict['pred_distill_loss']  # é¢„æµ‹è’¸é¦æŸå¤±
            
            if self.tie_feature_and_patch:
                # åˆå¹¶æ¨¡å¼
                if loss_dict['feature_distill_loss'].item() > 0:
                    total += self.beta * loss_dict['feature_distill_loss']
                
                # åˆå¹¶patchçº§åˆ«æŸå¤±
                patch_combined = 0.0
                if loss_dict['patch_feature_distill_loss'].item() > 0:
                    patch_combined += loss_dict['patch_feature_distill_loss']
                if loss_dict['patch_distance_distill_loss'].item() > 0:
                    patch_combined += loss_dict['patch_distance_distill_loss']
                
                if patch_combined > 0:
                    total += self.gamma * patch_combined
            else:
                # åˆ†ç¦»æ¨¡å¼
                if loss_dict['feature_distill_loss'].item() > 0:
                    total += self.beta * loss_dict['feature_distill_loss']
                if loss_dict['patch_feature_distill_loss'].item() > 0:
                    total += self.gamma * loss_dict['patch_feature_distill_loss']
                if loss_dict['patch_distance_distill_loss'].item() > 0:
                    total += self.gamma * loss_dict['patch_distance_distill_loss']
            
            loss_dict['total_loss'] = total
            # å°†ground truthæŸå¤±è®¾ä¸º0ï¼ˆç”¨äºè®°å½•ï¼‰
            loss_dict['gt_loss'] = torch.tensor(0.0, device=total.device)
        
        return loss_dict

class AblationDistillationTrainer(DistillationTrainer):
    """æ¶ˆèå®éªŒç‰ˆæœ¬çš„è’¸é¦è®­ç»ƒå™¨"""
    
    def __init__(self, teacher_model, student_model, train_loader, val_loader, 
                 config, device, save_path, task_type='regression', test_loader=None,
                 use_ground_truth=True):
        
        # å…ˆè°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼ˆä½†ä¸ä½¿ç”¨çˆ¶ç±»çš„distill_lossï¼‰
        super().__init__(teacher_model, student_model, train_loader, val_loader, 
                        config, device, save_path, task_type, test_loader)
        
        # åˆ›å»ºæ¶ˆèç‰ˆæœ¬çš„è’¸é¦æŸå¤±
        distill_config = config['distillation']
        self.distill_loss = AblationDistillationLoss(
            alpha=distill_config.get('alpha', 0.5),
            beta=distill_config.get('beta', 0.5),
            gamma=distill_config.get('gamma', 0.5),
            temperature=distill_config.get('temperature', 4.0),
            use_feature_distill=distill_config.get('use_feature_distill', True),
            feature_loss_weight=distill_config.get('feature_loss_weight', 1.0),
            use_patch_feature_distill=distill_config.get('use_patch_feature_distill', False),
            patch_feature_loss_weight=distill_config.get('patch_feature_loss_weight', 1.0),
            patch_distill_mode=distill_config.get('patch_distill_mode', 'direct'),
            contrastive_temperature=distill_config.get('contrastive_temperature', 0.1),
            use_patch_distance_distill=distill_config.get('use_patch_distance_distill', False),
            patch_distance_loss_weight=distill_config.get('patch_distance_loss_weight', 1.0),
            tie_feature_and_patch=distill_config.get('tie_feature_and_patch', True),
            use_ground_truth=use_ground_truth  # æ–°å¢å‚æ•°
        )
        
        # é‡æ–°è®¾ç½®ç‰¹å¾é€‚é…å™¨
        teacher_dim = self.get_feature_dim(teacher_model)
        student_dim = self.get_feature_dim(student_model)
        
        teacher_patch_dim = None
        student_patch_dim = None
        if self.distill_loss.use_patch_feature_distill:
            teacher_patch_dim = self.get_patch_feature_dim(teacher_model)
            student_patch_dim = self.get_patch_feature_dim(student_model)
        
        self.distill_loss.setup_feature_adapter(teacher_dim, student_dim, teacher_patch_dim, student_patch_dim)
        
        # å°†é€‚é…å™¨ç§»åŠ¨åˆ°è®¾å¤‡
        if self.distill_loss.feature_adapter is not None:
            self.distill_loss.feature_adapter = self.distill_loss.feature_adapter.to(device)
        
        if self.distill_loss.patch_distance_distiller is not None:
            self.distill_loss.patch_distance_distiller = self.distill_loss.patch_distance_distiller.to(device)
        
        # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆåŒ…å«æ–°çš„é€‚é…å™¨å‚æ•°ï¼‰
        optimizer_params = list(self.student_model.parameters())
        if self.distill_loss.feature_adapter is not None:
            optimizer_params.extend(self.distill_loss.feature_adapter.parameters())
        
        train_config = config['train_config']
        optimizer_type = train_config.get('optimizer', 'Adam').lower()
        lr = float(train_config.get('lr_init', 1e-5))
        weight_decay = float(train_config.get('weight_decay', 0.0))
        
        if optimizer_type == 'adam':
            self.optimizer = Adam(optimizer_params, lr=lr, weight_decay=weight_decay)
        elif optimizer_type == 'adamw':
            self.optimizer = AdamW(optimizer_params, lr=lr, weight_decay=weight_decay)

def main():
    parser = argparse.ArgumentParser(description='PPGæ¨¡å‹çŸ¥è¯†è’¸é¦æ¶ˆèå®éªŒ')
    parser.add_argument('--teacher_type', type=str, required=True, help='Teacheræ¨¡å‹ç±»å‹')
    parser.add_argument('--student_type', type=str, required=True, help='Studentæ¨¡å‹ç±»å‹')
    parser.add_argument('--dataset', type=str, required=True, help='æ•°æ®é›†åç§°')
    parser.add_argument('--teacher_path', type=str, help='Teacheræ¨¡å‹è·¯å¾„')
    parser.add_argument('--save_dir', type=str, default='./output', help='Teacheræ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--save_dir_student', type=str, default='./output_ablation', help='Studentæ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--config_path', type=str, required=True, help='è’¸é¦é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no_ground_truth', action='store_true', help='å»é™¤ground truthæŸå¤±')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # åŠ è½½è‡ªå®šä¹‰é…ç½®
    student_config_path = f"config/models/gpt_config_{args.dataset}.yaml"
    data_config_path = f"config/data/{args.dataset}_data.yaml"
    
    config, student_config, task_type = load_custom_config(
        args.config_path, student_config_path, data_config_path
    )
    
    # è‡ªåŠ¨æ¨æ–­teacheræ¨¡å‹è·¯å¾„
    if args.teacher_path is None:
        teacher_path = auto_infer_teacher_path(args.teacher_type, args.dataset, args.save_dir)
    else:
        teacher_path = args.teacher_path
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    lprint(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir_student, exist_ok=True)
    
    # æ„å»ºä¿å­˜è·¯å¾„
    config_name = os.path.splitext(os.path.basename(args.config_path))[0]
    save_path = os.path.join(args.save_dir_student, 
                           f"ablation_{config_name}_{args.teacher_type}_to_{args.student_type}_{args.dataset}")
    
    # æ•°æ®åŠ è½½
    data_cfg = config['data_config']
    patch_size = student_config.get('patch_size', 40)
    n_patches = calc_n_patches(data_cfg['train_data_path'], patch_size)
    student_config['n_patches'] = n_patches
    
    # åŠ è½½æ¨¡å‹
    teacher_model = load_teacher_model(teacher_path, args.teacher_type, device, n_patches)
    student_model = create_model(student_config, args.student_type)
    student_model = student_model.to(device)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = PretrainDataset(
        data_cfg['train_data_path'],
        patch_size=patch_size,
        train_labels_dataset_path=data_cfg.get('train_label_path', ''),
        data_red_factor=data_cfg.get('data_red_factor', 1)
    )
    
    val_dataset = None
    if data_cfg.get('val_data_path'):
        val_dataset = PretrainDataset(
            data_cfg['val_data_path'],
            patch_size=patch_size,
            train_labels_dataset_path=data_cfg.get('val_label_path', ''),
            data_red_factor=data_cfg.get('data_red_factor', 1)
        )
    
    test_dataset = None
    if data_cfg.get('test_data_path'):
        test_dataset = PretrainDataset(
            data_cfg['test_data_path'],
            patch_size=patch_size,
            train_labels_dataset_path=data_cfg.get('test_label_path', ''),
            data_red_factor=data_cfg.get('data_red_factor', 1)
        )
    
    train_config = config['train_config']
    train_loader = DataLoader(train_dataset, batch_size=train_config['batch_size'], shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=train_config['batch_size'], shuffle=False, num_workers=0) if val_dataset else None
    test_loader = DataLoader(test_dataset, batch_size=train_config['batch_size'], shuffle=False, num_workers=0) if test_dataset else None
    
    # åˆ›å»ºæ¶ˆèç‰ˆæœ¬çš„è®­ç»ƒå™¨
    trainer = AblationDistillationTrainer(
        teacher_model=teacher_model,
        student_model=student_model,
        train_loader=train_loader,
        val_loader=val_loader,
        config=config,
        device=device,
        save_path=save_path,
        task_type=task_type,
        test_loader=test_loader,
        use_ground_truth=not args.no_ground_truth  # æ§åˆ¶æ˜¯å¦ä½¿ç”¨ground truthæŸå¤±
    )
    
    # å¼€å§‹è®­ç»ƒ
    test_loss, test_metrics = trainer.train()
    
    if test_loader and test_loss is not None:
        lprint(f"\næœ€ç»ˆæµ‹è¯•æŸå¤±: {test_loss:.4f}")
        lprint(f"æœ€ç»ˆæµ‹è¯•æŒ‡æ ‡: {test_metrics}")

if __name__ == "__main__":
    main()
EOF

# ============================================================================
# åˆ›å»ºå®éªŒç‰¹å®šçš„è¾“å‡ºç›®å½•
# ============================================================================

exp_output_dir="$SAVE_DIR_STUDENT/${config_name}_${TEACHER_TYPE}_to_${STUDENT_TYPE}_${DATASET}"
mkdir -p "$exp_output_dir"

echo "âœ… åˆ›å»ºå®éªŒè¾“å‡ºç›®å½•: $exp_output_dir"

# ============================================================================
# è¿è¡ŒçŸ¥è¯†è’¸é¦æ¶ˆèå®éªŒ
# ============================================================================

echo ""
echo "=== å¼€å§‹æ¶ˆèå®éªŒ ==="
echo "å®éªŒ: $exp_description"
echo "é…ç½®: $config_name"
echo "æ¨¡å‹: $TEACHER_TYPE -> $STUDENT_TYPE"
echo "æ•°æ®é›†: $DATASET"
echo "è¾“å‡ºç›®å½•: $exp_output_dir"

# è®¾ç½®é¢å¤–çš„å‚æ•°ï¼ˆå¦‚æœæ˜¯å»é™¤ground truthå®éªŒï¼‰
extra_args=""
if [ "$config_name" = "no_ground_truth" ]; then
    extra_args="--no_ground_truth"
fi

python train_distill_ablation.py \
    --teacher_type $TEACHER_TYPE \
    --student_type $STUDENT_TYPE \
    --dataset $DATASET \
    --teacher_path $teacher_model_path \
    --save_dir $SAVE_DIR \
    --save_dir_student "$exp_output_dir" \
    --seed $SEED \
    --config_path $TEMP_CONFIG_DIR/${config_name}.yaml \
    $extra_args

training_exit_code=$?
echo ""
echo "å®Œæˆæ—¶é—´: $(date)"

# ============================================================================
# æ£€æŸ¥è®­ç»ƒç»“æœ
# ============================================================================

if [ $training_exit_code -eq 0 ]; then
    echo "âœ… æ¶ˆèå®éªŒæˆåŠŸå®Œæˆ!"
    echo ""
    
    # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
    student_model_file="$exp_output_dir/ablation_${config_name}_${TEACHER_TYPE}_to_${STUDENT_TYPE}_${DATASET}_best.pth"
    log_file="$exp_output_dir/ablation_${config_name}_${TEACHER_TYPE}_to_${STUDENT_TYPE}_${DATASET}_distill_training_log.json"
    metrics_file="$exp_output_dir/ablation_${config_name}_${TEACHER_TYPE}_to_${STUDENT_TYPE}_${DATASET}_distill_metrics.csv"
    
    echo "=== ç”Ÿæˆæ–‡ä»¶æ£€æŸ¥ ==="
    if [ -f "$student_model_file" ]; then
        student_size=$(du -h "$student_model_file" | cut -f1)
        echo "âœ… Studentæ¨¡å‹: $student_model_file (å¤§å°: $student_size)"
        
        # è®¡ç®—æ¨¡å‹å¤§å°æ¯”è¾ƒ
        teacher_size_bytes=$(stat -f%z "$teacher_model_path" 2>/dev/null || stat -c%s "$teacher_model_path" 2>/dev/null)
        student_size_bytes=$(stat -f%z "$student_model_file" 2>/dev/null || stat -c%s "$student_model_file" 2>/dev/null)
        if [ -n "$teacher_size_bytes" ] && [ -n "$student_size_bytes" ] && [ $teacher_size_bytes -gt 0 ]; then
            compression_ratio=$(echo "scale=2; $teacher_size_bytes / $student_size_bytes" | bc 2>/dev/null || echo "è®¡ç®—å¤±è´¥")
            echo "ğŸ“Š æ–‡ä»¶å‹ç¼©æ¯”: ${compression_ratio}x (${teacher_size} -> ${student_size})"
        fi
    else
        echo "âŒ Studentæ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: $student_model_file"
    fi
    
    if [ -f "$log_file" ]; then
        echo "âœ… è®­ç»ƒæ—¥å¿—: $log_file"
        
        # æå–å…³é”®ç»“æœä¿¡æ¯
        echo ""
        echo "=== å®éªŒç»“æœæ‘˜è¦ ==="
        ablation_result=$(python3 -c "
import json
try:
    with open('$log_file', 'r') as f:
        data = json.load(f)
    
    exp_info = data.get('experiment_info', {})
    test_results = data.get('test_results', {})
    
    # å‹ç¼©æ¯”ä¿¡æ¯
    compression_ratio = exp_info.get('compression_ratio', 'N/A')
    teacher_params = exp_info.get('teacher_params', {}).get('total', 'N/A')
    student_params = exp_info.get('student_params', {}).get('total', 'N/A')
    
    print(f'å‚æ•°å‹ç¼©æ¯”: {compression_ratio}x')
    if isinstance(teacher_params, int) and isinstance(student_params, int):
        print(f'å‚æ•°æ•°é‡: {teacher_params:,} -> {student_params:,}')
    
    # è®­ç»ƒä¿¡æ¯
    best_epoch = exp_info.get('best_epoch', 'N/A')
    best_val_loss = exp_info.get('best_val_loss', 'N/A')
    early_stopped = exp_info.get('early_stopped', False)
    
    print(f'æœ€ä½³epoch: {best_epoch}')
    print(f'æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}' if isinstance(best_val_loss, (int, float)) else f'æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss}')
    print(f'æ—©åœ: {\"æ˜¯\" if early_stopped else \"å¦\"}')
    
    # æµ‹è¯•ç»“æœ
    if 'test_loss' in test_results:
        print(f'æµ‹è¯•æŸå¤±: {test_results[\"test_loss\"]:.6f}')
        test_metrics = test_results.get('test_metrics', {})
        if 'mse' in test_metrics:
            print(f'æµ‹è¯•MSE: {test_metrics[\"mse\"]:.6f}')
        if 'mae' in test_metrics:
            print(f'æµ‹è¯•MAE: {test_metrics[\"mae\"]:.6f}')
    else:
        print('æµ‹è¯•ç»“æœ: æœªæ‰¾åˆ°')
        
except Exception as e:
    print(f'è¯»å–ç»“æœå¤±è´¥: {e}')
" 2>/dev/null)
        echo "$ablation_result" | sed 's/^/  /'
    else
        echo "âŒ è®­ç»ƒæ—¥å¿—æœªæ‰¾åˆ°: $log_file"
    fi
    
    if [ -f "$metrics_file" ]; then
        echo "âœ… æŒ‡æ ‡CSV: $metrics_file"
    else
        echo "âŒ æŒ‡æ ‡CSVæœªæ‰¾åˆ°: $metrics_file"
    fi
    
else
    echo "âŒ æ¶ˆèå®éªŒå¤±è´¥ (é€€å‡ºä»£ç : $training_exit_code)"
    echo "å®éªŒ: $exp_description"
    echo "é…ç½®: $config_name"
fi

# ============================================================================
# æ¸…ç†å’Œå®Œæˆ
# ============================================================================

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -rf $TEMP_CONFIG_DIR
rm -f train_distill_ablation.py

echo ""
echo "=== å®éªŒå®Œæˆ ==="
echo "å®éªŒ: $exp_description"
echo "é…ç½®: $config_name"
echo "æ¨¡å‹: $TEACHER_TYPE -> $STUDENT_TYPE"
echo "æ•°æ®é›†: $DATASET"
echo "è¾“å‡ºç›®å½•: $exp_output_dir" 