#!/usr/bin/env python
"""
ç»Ÿä¸€çš„PPGæ¨¡å‹çŸ¥è¯†è’¸é¦è®­ç»ƒè„šæœ¬
æ”¯æŒGPTã€Linearã€MLPä½œä¸ºteacheræˆ–studentæ¨¡å‹çš„å„ç§ç»„åˆ
"""

import os
os.environ['NUMEXPR_MAX_THREADS'] = '16'
import sys
sys.path.append('.')
import argparse
import torch
import logging
logging.basicConfig(level=logging.INFO)
from logging import info as lprint
import yaml
import json
import numpy as np
import tqdm
import random
import time
import math
from sklearn.metrics import mean_squared_error, mean_absolute_error
from datetime import datetime
import csv

from torch.utils.data import DataLoader
from torch.optim import Adam, AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR
import torch.nn as nn
import torch.nn.functional as F

# å¯¼å…¥è·ç¦»è®¡ç®—å‡½æ•°
def pdist(e, squared=False, eps=1e-12):
    """è®¡ç®—æˆå¯¹è·ç¦»"""
    e_square = e.pow(2).sum(dim=1)
    prod = e @ e.t()
    res = (e_square.unsqueeze(1) + e_square.unsqueeze(0) - 2 * prod).clamp(min=eps)
    
    if not squared:
        res = res.sqrt()
    
    res = res.clone()
    res[range(len(e)), range(len(e))] = 0
    return res


class RkdDistance(nn.Module):
    """å…³ç³»çŸ¥è¯†è’¸é¦ - è·ç¦»æŸå¤±"""
    def forward(self, student, teacher):
        with torch.no_grad():
            t_d = pdist(teacher, squared=False)
            mean_td = t_d[t_d>0].mean()
            t_d = t_d / mean_td

        d = pdist(student, squared=False)
        mean_d = d[d>0].mean()
        d = d / mean_d

        loss = F.smooth_l1_loss(d, t_d, reduction='mean')
        return loss


class LrScheduler:
    """è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œæ”¯æŒwarmup + cosine annealing"""
    def __init__(self, optimizer, warmup_scheduler, main_scheduler, warmup_steps, total_steps):
        self.optimizer = optimizer
        self.warmup_scheduler = warmup_scheduler
        self.main_scheduler = main_scheduler
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.step_count = 0

    def get_last_lr(self):
        if self.step_count < self.warmup_steps:
            return self.warmup_scheduler.get_last_lr()
        else:
            return self.main_scheduler.get_last_lr()

    def step(self, epoch=None):
        if self.step_count < self.warmup_steps:
            self.warmup_scheduler.step()
        else:
            self.main_scheduler.step()
        self.step_count += 1

    def load_state_dict(self, state_dict):
        self.step_count = state_dict['step_count']
        self.warmup_scheduler.load_state_dict(state_dict['warmup_scheduler'])
        self.main_scheduler.load_state_dict(state_dict['main_scheduler'])

    def state_dict(self):
        return {
            'step_count': self.step_count,
            'warmup_scheduler': self.warmup_scheduler.state_dict(),
            'main_scheduler': self.main_scheduler.state_dict(),
        }

# å¯¼å…¥æ¨¡å‹
from model.gpt import GPT_with_linearOutput
from model.linear import LinearModel, create_linear_model
from model.mlp import MLP, create_mlp_model
from model.papagei import PapageiModel, create_papagei_model, count_papagei_parameters
from data.pretrain_dataset import PretrainDataset
try:
    from local.supp_fxns import *
except ImportError:
    pass  # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨å†…å»ºåŠŸèƒ½


def calc_n_patches(filepath, fs):
    """è®¡ç®—æ•°æ®çš„patchæ•°é‡"""
    assert os.path.isfile(filepath), f'{filepath=}'
    seq_length = get_numpy_array_metadata(filepath, return_attrs=True)[0][-1]
    assert seq_length >= fs, 'need atleast 1 second signal length'
    assert seq_length % fs == 0, 'signal length (in seconds) must be a whole number'
    n_patches = seq_length // fs
    return n_patches


class DistillationLoss(nn.Module):
    """çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°"""
    
    def __init__(self, alpha=0.5, beta=0.5, gamma=0.5, temperature=4.0, 
                 use_feature_distill=True, feature_loss_weight=1.0,
                 use_patch_feature_distill=False, patch_feature_loss_weight=1.0,
                 patch_distill_mode='direct', contrastive_temperature=0.1,
                 use_patch_distance_distill=False, patch_distance_loss_weight=1.0,
                 # é™å‚ä¼˜åŒ–é€‰é¡¹
                 tie_feature_and_patch=True):    # åˆå¹¶è¡¨å¾è’¸é¦
        """
        Args:
            alpha: é¢„æµ‹è’¸é¦æŸå¤±æƒé‡ï¼ˆå¯æ‰«å‚æ•°ï¼‰
            beta: å…¨å±€ç‰¹å¾è’¸é¦æŸå¤±æƒé‡ï¼ˆå¯æ‰«å‚æ•°ï¼‰
            gamma: patchçº§åˆ«è’¸é¦æŸå¤±æƒé‡ï¼ˆå¯æ‰«å‚æ•°ï¼ŒåŒ…å«patch_feature_losså’Œpatch_distance_lossï¼‰
            temperature: è½¯åŒ–æ¸©åº¦
            use_feature_distill: æ˜¯å¦ä½¿ç”¨å…¨å±€ç‰¹å¾è’¸é¦
            feature_loss_weight: å…¨å±€ç‰¹å¾è’¸é¦æŸå¤±æƒé‡
            use_patch_feature_distill: æ˜¯å¦ä½¿ç”¨patchçº§åˆ«ç‰¹å¾è’¸é¦
            patch_feature_loss_weight: patchç‰¹å¾è’¸é¦æŸå¤±æƒé‡
            patch_distill_mode: patchè’¸é¦æ¨¡å¼ ('direct' æˆ– 'contrastive')
            contrastive_temperature: å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•° (æˆ– 'auto')
            use_patch_distance_distill: æ˜¯å¦ä½¿ç”¨patchè·ç¦»è’¸é¦
            patch_distance_loss_weight: patchè·ç¦»è’¸é¦æŸå¤±æƒé‡
            tie_feature_and_patch: æ˜¯å¦åˆå¹¶è¡¨å¾è’¸é¦
        """
        super(DistillationLoss, self).__init__()
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.temperature = temperature
        self.use_feature_distill = use_feature_distill
        self.feature_loss_weight = feature_loss_weight
        self.use_patch_feature_distill = use_patch_feature_distill
        self.patch_feature_loss_weight = patch_feature_loss_weight
        self.patch_distill_mode = patch_distill_mode
        self.contrastive_temperature = contrastive_temperature
        self.use_patch_distance_distill = use_patch_distance_distill
        self.patch_distance_loss_weight = patch_distance_loss_weight
        
        # é™å‚ä¼˜åŒ–é€‰é¡¹
        self.tie_feature_and_patch = tie_feature_and_patch
        
        # ç‰¹å¾é€‚é…å™¨ï¼ˆå…¨å±€å’Œpatchç‰¹å¾å…±ç”¨ï¼Œå› ä¸ºéƒ½æ¥è‡ªåŒä¸€ä¸ªbackboneï¼‰
        self.feature_adapter = None
        
        # patchè·ç¦»è’¸é¦å™¨
        self.patch_distance_distiller = RkdDistance() if use_patch_distance_distill else None
        
    def setup_feature_adapter(self, teacher_dim, student_dim, teacher_patch_dim=None, student_patch_dim=None):
        """è®¾ç½®ç‰¹å¾é€‚é…å™¨ï¼Œå…¨å±€å’Œpatchç‰¹å¾å…±ç”¨åŒä¸€ä¸ªé€‚é…å™¨ï¼ˆå› ä¸ºç»´åº¦ç›¸åŒï¼‰"""
        
        # æ£€æŸ¥ç»´åº¦ä¸€è‡´æ€§
        if teacher_patch_dim is not None and teacher_patch_dim != teacher_dim:
            lprint(f"è­¦å‘Š: teacherå…¨å±€ç‰¹å¾ç»´åº¦({teacher_dim})ä¸patchç‰¹å¾ç»´åº¦({teacher_patch_dim})ä¸åŒ¹é…")
        
        if student_patch_dim is not None and student_patch_dim != student_dim:
            lprint(f"è­¦å‘Š: studentå…¨å±€ç‰¹å¾ç»´åº¦({student_dim})ä¸patchç‰¹å¾ç»´åº¦({student_patch_dim})ä¸åŒ¹é…")
        
        # åªè¦teacherå’Œstudentçš„ç‰¹å¾ç»´åº¦ä¸åŒ¹é…å°±éœ€è¦é€‚é…å™¨
        need_adapter = (
            (self.use_feature_distill and teacher_dim != student_dim) or
            (self.use_patch_feature_distill and teacher_dim != student_dim)
        )
        
        if need_adapter:
            self.feature_adapter = nn.Linear(teacher_dim, student_dim)
            lprint(f"åˆ›å»ºå…±äº«ç‰¹å¾é€‚é…å™¨: {teacher_dim} -> {student_dim} (å…¨å±€å’Œpatchç‰¹å¾å…±ç”¨)")
        else:
            lprint("ç‰¹å¾ç»´åº¦åŒ¹é…ï¼Œæ— éœ€é€‚é…å™¨")
    

        
    def forward(self, student_output, teacher_output, student_features=None, 
                teacher_features=None, targets=None, task_type='regression',
                student_patch_features=None, teacher_patch_features=None):
        """
        è®¡ç®—è’¸é¦æŸå¤±ï¼ˆæ”¯æŒé™å‚ä¼˜åŒ–ï¼‰
        
        Args:
            student_output: å­¦ç”Ÿæ¨¡å‹è¾“å‡º
            teacher_output: æ•™å¸ˆæ¨¡å‹è¾“å‡º  
            student_features: å­¦ç”Ÿæ¨¡å‹å…¨å±€ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            teacher_features: æ•™å¸ˆæ¨¡å‹å…¨å±€ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            targets: çœŸå®æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
            task_type: ä»»åŠ¡ç±»å‹
            student_patch_features: å­¦ç”Ÿæ¨¡å‹patchçº§åˆ«ç‰¹å¾ [B, n_patches, dim]ï¼ˆå¯é€‰ï¼‰
            teacher_patch_features: æ•™å¸ˆæ¨¡å‹patchçº§åˆ«ç‰¹å¾ [B, n_patches, dim]ï¼ˆå¯é€‰ï¼‰
        """
        
        # === æ­¥éª¤1: è®¡ç®—åŸå§‹æŸå¤±åˆ†é‡ ===
        gt_loss = None
        if targets is not None:
            if task_type == 'regression':
                gt_loss = F.mse_loss(student_output.squeeze(), targets)
            else:
                gt_loss = F.cross_entropy(student_output, targets.long())

        if task_type == 'regression':
            pred_loss = F.mse_loss(student_output, teacher_output)
        else:
            teacher_soft = F.softmax(teacher_output / self.temperature, dim=1)
            student_log_soft = F.log_softmax(student_output / self.temperature, dim=1)
            pred_loss = F.kl_div(student_log_soft, teacher_soft, reduction='batchmean')
            pred_loss = pred_loss * (self.temperature ** 2)

        # å…¨å±€ç‰¹å¾è’¸é¦æŸå¤±
        global_feat_loss = None
        if self.use_feature_distill and student_features is not None and teacher_features is not None:
            if len(teacher_features.shape) == 3:
                teacher_features = teacher_features.mean(dim=1)
            teacher_feat_adapted = self.feature_adapter(teacher_features) if self.feature_adapter is not None else teacher_features
            global_feat_loss = F.mse_loss(student_features, teacher_feat_adapted)

        # patchçº§åˆ«ç‰¹å¾è’¸é¦æŸå¤±
        patch_feat_loss = None
        if self.use_patch_feature_distill and student_patch_features is not None and teacher_patch_features is not None:
            # åº”ç”¨ç‰¹å¾é€‚é…å™¨ï¼ˆéœ€è¦æ­£ç¡®å¤„ç†3Då¼ é‡ï¼‰
            if self.feature_adapter is not None:
                teacher_patch_adapted = self.feature_adapter(teacher_patch_features)
            else:
                teacher_patch_adapted = teacher_patch_features
            
            if self.patch_distill_mode == 'direct':
                patch_feat_loss = F.mse_loss(student_patch_features, teacher_patch_adapted)
            else:
                patch_feat_loss = self._compute_contrastive_patch_loss(student_patch_features, teacher_patch_adapted)

        # patchè·ç¦»è’¸é¦æŸå¤±
        patch_distance_loss = None
        if self.use_patch_distance_distill and student_patch_features is not None and teacher_patch_features is not None:
            # å¯¹æ¯ä¸ªbatchä¸­çš„patchè¿›è¡Œè·ç¦»è’¸é¦
            batch_size = student_patch_features.size(0)
            patch_distance_losses = []
            
            for b in range(batch_size):
                student_patches_b = student_patch_features[b]  # [n_patches, dim]
                teacher_patches_b = teacher_patch_features[b]  # [n_patches, dim]
                
                # è®¡ç®—patchä¹‹é—´çš„è·ç¦»è’¸é¦æŸå¤±
                patch_dist_loss_b = self.patch_distance_distiller(student_patches_b, teacher_patches_b)
                patch_distance_losses.append(patch_dist_loss_b)
            
            patch_distance_loss = torch.stack(patch_distance_losses).mean()

        # === æ­¥éª¤2: ç›´æ¥ä½¿ç”¨åŸå§‹æŸå¤±ï¼ˆæ— EMAå½’ä¸€åŒ–ï¼‰ ===
        ngt = gt_loss
        npred = pred_loss
        nfeat = global_feat_loss
        npatch = patch_feat_loss
        npatch_distance = patch_distance_loss

        # === æ­¥éª¤3: åˆå¹¶patchçº§åˆ«è’¸é¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰ ===
        npatch_combined = None
        if self.tie_feature_and_patch:
            # åˆå¹¶patchçº§åˆ«çš„æŸå¤±
            patch_parts = [x for x in [npatch, npatch_distance] if x is not None]
            npatch_combined = sum(patch_parts) if patch_parts else None

        # === æ­¥éª¤4: ç»„åˆæŸå¤± ===
        loss_dict = {}
        
        # æ–°çš„æƒé‡æ¨¡å¼ï¼šgt_loss=1, pred_loss=alpha, global_feat_loss=beta, patch_loss=gamma
        total = 0.0
        if ngt is not None:
            total += 1.0 * ngt  # gt_lossæƒé‡å›ºå®šä¸º1
        total += self.alpha * npred  # pred_lossæƒé‡ä¸ºalpha
        
        if self.tie_feature_and_patch:
            # åˆå¹¶æ¨¡å¼ï¼šå…¨å±€ç‰¹å¾ç”¨betaï¼Œpatchçº§åˆ«ç”¨gamma
            if nfeat is not None:
                total += self.beta * nfeat  # å…¨å±€ç‰¹å¾è’¸é¦æƒé‡ä¸ºbeta
            if npatch_combined is not None:
                total += self.gamma * npatch_combined  # patchçº§åˆ«è’¸é¦æƒé‡ä¸ºgamma
        else:
            # åˆ†ç¦»æ¨¡å¼ï¼šå„è‡ªç‹¬ç«‹æƒé‡
            if nfeat is not None:
                total += self.beta * nfeat  # å…¨å±€ç‰¹å¾è’¸é¦æƒé‡ä¸ºbeta
            if npatch is not None:
                total += self.gamma * npatch  # patchç‰¹å¾è’¸é¦æƒé‡ä¸ºgamma
            if npatch_distance is not None:
                total += self.gamma * npatch_distance  # patchè·ç¦»è’¸é¦æƒé‡ä¸ºgamma

        loss_dict['gt_loss'] = ngt if ngt is not None else torch.tensor(0.0, device=npred.device)
        loss_dict['pred_distill_loss'] = npred
        loss_dict['feature_distill_loss'] = nfeat if nfeat is not None else torch.tensor(0.0, device=npred.device)
        loss_dict['patch_feature_distill_loss'] = npatch if npatch is not None else torch.tensor(0.0, device=npred.device)
        loss_dict['patch_distance_distill_loss'] = npatch_distance if npatch_distance is not None else torch.tensor(0.0, device=npred.device)
        loss_dict['total_loss'] = total
        return loss_dict
    
    def _compute_patch_distillation_loss(self, student_patch_features, teacher_patch_features):
        """
        è®¡ç®—patchçº§åˆ«ç‰¹å¾è’¸é¦æŸå¤±
        
        Args:
            student_patch_features: [B, n_patches, dim_s]
            teacher_patch_features: [B, n_patches, dim_t]
            
        Returns:
            patch_distill_loss: æ ‡é‡æŸå¤±å€¼
        """
        # åº”ç”¨ç‰¹å¾é€‚é…å™¨ï¼ˆéœ€è¦æ­£ç¡®å¤„ç†3Då¼ é‡ï¼‰
        if self.feature_adapter is not None:
            # teacher_patch_features: [B, n_patches, teacher_dim] -> [B, n_patches, student_dim]
            B, n_patches, teacher_dim = teacher_patch_features.shape
            # é‡å¡‘ä¸º2Då¼ é‡ä»¥åº”ç”¨çº¿æ€§å˜æ¢
            teacher_patch_flat = teacher_patch_features.view(B * n_patches, teacher_dim)
            teacher_patch_adapted_flat = self.feature_adapter(teacher_patch_flat)
            # é‡å¡‘å›3Då¼ é‡
            student_dim = teacher_patch_adapted_flat.shape[-1]
            teacher_patch_features_adapted = teacher_patch_adapted_flat.view(B, n_patches, student_dim)
        else:
            teacher_patch_features_adapted = teacher_patch_features
        
        if self.patch_distill_mode == 'direct':
            # ç›´æ¥åŒ¹é…æ¨¡å¼ï¼šå¯¹åº”patchä¹‹é—´è®¡ç®—MSEæŸå¤±
            patch_distill_loss = F.mse_loss(student_patch_features, teacher_patch_features_adapted)
            
        elif self.patch_distill_mode == 'contrastive':
            # å¯¹æ¯”å­¦ä¹ æ¨¡å¼ï¼šå¯¹åº”patchä¸ºæ­£æ ·æœ¬ï¼Œå…¶ä»–patchä¸ºè´Ÿæ ·æœ¬
            patch_distill_loss = self._compute_contrastive_patch_loss(
                student_patch_features, teacher_patch_features_adapted
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„patchè’¸é¦æ¨¡å¼: {self.patch_distill_mode}")
        
        return patch_distill_loss
    
    def _compute_contrastive_patch_loss(self, student_patches, teacher_patches):
        """
        è®¡ç®—å¯¹æ¯”å­¦ä¹ çš„patchè’¸é¦æŸå¤±
        
        Args:
            student_patches: [B, n_patches, dim]
            teacher_patches: [B, n_patches, dim]
            
        Returns:
            contrastive_loss: æ ‡é‡æŸå¤±å€¼
        """
        batch_size, n_patches, dim = student_patches.shape
        
        # è‡ªåŠ¨æ¸©åº¦è®¾ç½®
        if isinstance(self.contrastive_temperature, str) and self.contrastive_temperature == 'auto':
            tau = 1.0 / math.sqrt(dim)
        else:
            tau = self.contrastive_temperature
        
        # å°†studentå’Œteacher patchç‰¹å¾å½’ä¸€åŒ–
        student_patches_norm = F.normalize(student_patches, dim=-1)  # [B, n_patches, dim]
        teacher_patches_norm = F.normalize(teacher_patches, dim=-1)  # [B, n_patches, dim]
        
        total_loss = 0.0
        
        # å¯¹æ¯ä¸ªbatchä¸­çš„æ¯ä¸ªpatchè¿›è¡Œå¯¹æ¯”å­¦ä¹ 
        for b in range(batch_size):
            student_b = student_patches_norm[b]  # [n_patches, dim]
            teacher_b = teacher_patches_norm[b]   # [n_patches, dim]
            
            # è®¡ç®—student patchä¸æ‰€æœ‰teacher patchçš„ç›¸ä¼¼åº¦
            # similarities: [n_patches, n_patches]
            similarities = torch.mm(student_b, teacher_b.t()) / tau
            
            # å¯¹è§’çº¿å…ƒç´ ä¸ºæ­£æ ·æœ¬ï¼ˆå¯¹åº”patchï¼‰ï¼Œå…¶ä½™ä¸ºè´Ÿæ ·æœ¬
            labels = torch.arange(n_patches, device=similarities.device)
            
            # ä½¿ç”¨äº¤å‰ç†µæŸå¤±
            contrastive_loss_b = F.cross_entropy(similarities, labels)
            total_loss += contrastive_loss_b
        
        # å¹³å‡æ‰€æœ‰batchçš„æŸå¤±
        return total_loss / batch_size


class DistillationTrainer:
    """çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨"""
    
    def __init__(self, teacher_model, student_model, train_loader, val_loader, 
                 config, device, save_path, task_type='regression', test_loader=None):
        self.teacher_model = teacher_model
        self.student_model = student_model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.config = config
        self.device = device
        self.save_path = save_path
        self.task_type = task_type
        
        # åˆ›å»ºè’¸é¦æŸå¤±
        distill_config = config['distillation']
        self.distill_loss = DistillationLoss(
            alpha=distill_config.get('alpha', 0.5),
            beta=distill_config.get('beta', 0.5),
            gamma=distill_config.get('gamma', 0.5),
            temperature=distill_config.get('temperature', 4.0),
            use_feature_distill=distill_config.get('use_feature_distill', True),
            feature_loss_weight=distill_config.get('feature_loss_weight', 1.0),
            use_patch_feature_distill=distill_config.get('use_patch_feature_distill', False),
            patch_feature_loss_weight=distill_config.get('patch_feature_loss_weight', 1.0),
            patch_distill_mode=distill_config.get('patch_distill_mode', 'direct'),
            contrastive_temperature=distill_config.get('contrastive_temperature', 0.1),
            use_patch_distance_distill=distill_config.get('use_patch_distance_distill', False),
            patch_distance_loss_weight=distill_config.get('patch_distance_loss_weight', 1.0),
            # é™å‚ä¼˜åŒ–é€‰é¡¹
            tie_feature_and_patch=distill_config.get('tie_feature_and_patch', True)
        )
        
        # è®¾ç½®ç‰¹å¾é€‚é…å™¨
        teacher_dim = self.get_feature_dim(teacher_model)
        student_dim = self.get_feature_dim(student_model)
        
        # è·å–patchçº§åˆ«ç‰¹å¾ç»´åº¦ï¼ˆä»…å¯¹GPTæ¨¡å‹ï¼‰
        teacher_patch_dim = None
        student_patch_dim = None
        if self.distill_loss.use_patch_feature_distill:
            teacher_patch_dim = self.get_patch_feature_dim(teacher_model)
            student_patch_dim = self.get_patch_feature_dim(student_model)
        
        self.distill_loss.setup_feature_adapter(teacher_dim, student_dim, teacher_patch_dim, student_patch_dim)
        
        # å°†ç‰¹å¾é€‚é…å™¨ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        if self.distill_loss.feature_adapter is not None:
            self.distill_loss.feature_adapter = self.distill_loss.feature_adapter.to(device)
            lprint(f"ç‰¹å¾é€‚é…å™¨å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
        
        # å°†patchè·ç¦»è’¸é¦å™¨ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        if self.distill_loss.patch_distance_distiller is not None:
            self.distill_loss.patch_distance_distiller = self.distill_loss.patch_distance_distiller.to(device)
            lprint(f"patchè·ç¦»è’¸é¦å™¨å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨å‚æ•°åˆ—è¡¨
        optimizer_params = list(self.student_model.parameters())
        
        # æ·»åŠ å…±äº«çš„ç‰¹å¾é€‚é…å™¨å‚æ•°
        if self.distill_loss.feature_adapter is not None:
            optimizer_params.extend(self.distill_loss.feature_adapter.parameters())
            lprint("ç‰¹å¾é€‚é…å™¨å‚æ•°å·²åŠ å…¥ä¼˜åŒ–å™¨")
        
        # ä¼˜åŒ–å™¨
        train_config = config['train_config']
        optimizer_type = train_config.get('optimizer', 'Adam').lower()
        # ä½¿ç”¨åˆå§‹å­¦ä¹ ç‡ä½œä¸ºä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡ï¼Œåç»­é€šè¿‡è°ƒåº¦å™¨è°ƒæ•´
        lr = float(train_config.get('lr_init', 1e-5))
        weight_decay = float(train_config.get('weight_decay', 0.0))
        
        if optimizer_type == 'adam':
            self.optimizer = Adam(optimizer_params, lr=lr, weight_decay=weight_decay)
        elif optimizer_type == 'adamw':
            self.optimizer = AdamW(optimizer_params, lr=lr, weight_decay=weight_decay)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - æŒ‰batchç²’åº¦è°ƒåº¦
        self.scheduler = None
        
        # è·å–å­¦ä¹ ç‡ç›¸å…³é…ç½®ï¼Œè®¾ç½®é»˜è®¤å€¼å¹¶ç¡®ä¿ç±»å‹æ­£ç¡®
        lr_init = float(train_config.get('lr_init', 1e-5))
        lr_max = float(train_config.get('lr_max', 30e-5))
        lr_final = float(train_config.get('lr_final', 1e-6))
        lr_schedule_ratio = float(train_config.get('lr_schedule_ratio', 1))
        lr_warm_up = float(train_config.get('lr_warm_up', 0.25))
        
        # è®¡ç®—æ€»çš„è°ƒåº¦æ­¥æ•°ï¼ˆæŒ‰batchè®¡ç®—ï¼‰
        epochs = train_config['epochs']
        batches_per_epoch = len(train_loader)
        lr_schedule_step = int(lr_schedule_ratio * epochs * batches_per_epoch)
        warm_up_steps = int(lr_schedule_step * lr_warm_up)
        
        lprint(f"å­¦ä¹ ç‡è°ƒåº¦é…ç½®:")
        lprint(f"  lr_init: {lr_init}, lr_max: {lr_max}, lr_final: {lr_final}")
        lprint(f"  æ€»è°ƒåº¦æ­¥æ•°: {lr_schedule_step}, é¢„çƒ­æ­¥æ•°: {warm_up_steps}")
        lprint(f"  æ¯epochæ‰¹æ¬¡æ•°: {batches_per_epoch}")
        
        # åˆ›å»ºé¢„çƒ­è°ƒåº¦å™¨
        lambda_warmup = lambda step: (lr_init + step * (lr_max - lr_init) / warm_up_steps) / lr_init
        warm_up_scheduler = LambdaLR(self.optimizer, lr_lambda=lambda_warmup)
        
        # åˆ›å»ºä¸»è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰
        main_scheduler = CosineAnnealingLR(self.optimizer, T_max=lr_schedule_step - warm_up_steps, eta_min=lr_final)
        
        # åˆ›å»ºç»„åˆè°ƒåº¦å™¨
        self.scheduler = LrScheduler(self.optimizer, warm_up_scheduler, main_scheduler, warm_up_steps, lr_schedule_step)
        
        lprint(f"å·²åˆ›å»ºæŒ‰batchç²’åº¦çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ (warmup + cosine annealing)")
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_loss = float('inf')
        self.best_epoch = 0
        
        # æ—©åœå‚æ•°
        self.early_stop_patience = train_config.get('early_stop_patience', 20)  # é»˜è®¤20ä¸ªepochæ— æ”¹å–„å°±åœæ­¢
        self.early_stop_min_delta = float(train_config.get('early_stop_min_delta', 1e-6))  # æœ€å°æ”¹å–„é˜ˆå€¼
        self.early_stop_counter = 0
        self.early_stopped = False
        
        # æ€§èƒ½ç»Ÿè®¡
        self.all_train_batch_times = []
        self.all_val_batch_times = []
        self.all_test_batch_times = []
        
    def get_feature_dim(self, model):
        """è·å–æ¨¡å‹ç‰¹å¾ç»´åº¦ - é€šè¿‡å®é™…å‰å‘ä¼ æ’­æ¢æµ‹"""
        # ä½¿ç”¨ä¸€ä¸ªå°batchçš„æ•°æ®æ¥æ¢æµ‹ç‰¹å¾ç»´åº¦
        model.eval()
        with torch.no_grad():
            # è·å–ä¸€ä¸ªå°æ‰¹æ¬¡çš„æ•°æ®è¿›è¡Œæ¢æµ‹
            probe = next(iter(self.train_loader))["ppg_segments"].to(self.device)[:2]
            features = self.get_features(model, probe)
            
            if features.dim() == 3:
                feature_dim = features.shape[-1]
            else:
                feature_dim = features.shape[-1]
            
            lprint(f"æ¢æµ‹åˆ°æ¨¡å‹å…¨å±€ç‰¹å¾ç»´åº¦: {feature_dim}")
            return feature_dim
    
    def get_patch_feature_dim(self, model):
        """è·å–æ¨¡å‹patchçº§åˆ«ç‰¹å¾ç»´åº¦ - é€‚ç”¨äºGPTæ¨¡å‹å’ŒPapageiæ¨¡å‹"""
        if hasattr(model, 'gpt'):
            # GPTæ¨¡å‹
            model.eval()
            with torch.no_grad():
                # è·å–ä¸€ä¸ªå°æ‰¹æ¬¡çš„æ•°æ®è¿›è¡Œæ¢æµ‹
                probe = next(iter(self.train_loader))["ppg_segments"].to(self.device)[:2]
                patch_features = self.get_patch_features(model, probe)
                
                if patch_features is None:
                    return None
                
                patch_feature_dim = patch_features.shape[-1]  # [B, n_patches, dim]
                lprint(f"æ¢æµ‹åˆ°GPTæ¨¡å‹patchç‰¹å¾ç»´åº¦: {patch_feature_dim}")
                return patch_feature_dim
        elif isinstance(model, PapageiModel):
            # Papageiæ¨¡å‹
            model.eval()
            with torch.no_grad():
                # è·å–ä¸€ä¸ªå°æ‰¹æ¬¡çš„æ•°æ®è¿›è¡Œæ¢æµ‹
                probe = next(iter(self.train_loader))["ppg_segments"].to(self.device)[:2]
                patch_features = self.get_patch_features(model, probe)
                
                if patch_features is None:
                    return None
                
                patch_feature_dim = patch_features.shape[-1]  # [B, n_patches, dim]
                lprint(f"æ¢æµ‹åˆ°Papageiæ¨¡å‹patchç‰¹å¾ç»´åº¦: {patch_feature_dim}")
                return patch_feature_dim
        else:
            lprint(f"æ¨¡å‹ {type(model).__name__} ä¸æ”¯æŒpatchçº§åˆ«ç‰¹å¾è’¸é¦")
            return None
    
    def get_features(self, model, x):
        """è·å–æ¨¡å‹çš„ç‰¹å¾è¡¨ç¤º"""
        if isinstance(model, PapageiModel):
            # Papageiæ¨¡å‹ï¼šä½¿ç”¨ä¸“é—¨çš„å…¨å±€ç‰¹å¾æå–æ–¹æ³•
            if model == self.teacher_model:
                with torch.no_grad():
                    features = model.get_global_features(x)
            else:
                features = model.get_global_features(x)
            return features
        elif hasattr(model, 'gpt'):
            # GPTæ¨¡å‹ï¼šè·å–ç¼–ç ç‰¹å¾
            if model == self.teacher_model:
                with torch.no_grad():
                    features = model.gpt.encode(x, apply_mask=False)
            else:
                features = model.gpt.encode(x, apply_mask=False)
            
            if features.dim() == 3:
                features = features.mean(dim=1)  # å¹³å‡æ± åŒ– [B, S, D] -> [B, D]
            return features
        elif hasattr(model, 'get_features'):
            # å¦‚æœæ¨¡å‹æœ‰ä¸“é—¨çš„ç‰¹å¾æå–æ–¹æ³•
            return model.get_features(x)
        else:
            # Linear/MLPæ¨¡å‹ï¼šä½¿ç”¨ä¸­é—´å±‚è¾“å‡ºä½œä¸ºç‰¹å¾
            batch_size = x.size(0)
            x_flat = x.view(batch_size, -1)
            
            if hasattr(model, 'layers') and len(model.layers) > 1:
                # MLPæ¨¡å‹ï¼šè·å–æœ€åä¸€ä¸ªéšè—å±‚
                features = x_flat
                for i, layer in enumerate(model.layers[:-1]):
                    features = layer(features)
                    if hasattr(model, 'activation') and i < len(model.layers) - 2:
                        features = model.activation(features)
                return features
            else:
                # Linearæ¨¡å‹ï¼šç›´æ¥è¿”å›è¾“å…¥ç‰¹å¾
                return x_flat
    
    def get_patch_features(self, model, x):
        """è·å–æ¨¡å‹çš„patchçº§åˆ«ç‰¹å¾ - é€‚ç”¨äºGPTæ¨¡å‹å’ŒPapageiæ¨¡å‹"""
        if hasattr(model, 'gpt'):
            # GPTæ¨¡å‹
            if model == self.teacher_model:
                with torch.no_grad():
                    # é€šè¿‡GPTç¼–ç å™¨è·å–patchçº§åˆ«ç‰¹å¾
                    patch_features = model.gpt.encode(x, apply_mask=False)  # [B, n_patches+1, dim] (åŒ…å«SEP token)
            else:
                patch_features = model.gpt.encode(x, apply_mask=False)  # [B, n_patches+1, dim] (åŒ…å«SEP token)
            
            # GPTçš„PatchEmbeddingä¼šåœ¨å‰é¢æ·»åŠ ä¸€ä¸ªSEP tokenï¼Œéœ€è¦å»æ‰ä»¥åŒ¹é…å®é™…çš„patchæ•°é‡
            if patch_features.size(1) > x.size(1):  # å¦‚æœç‰¹å¾æ•°é‡å¤§äºè¾“å…¥patchæ•°é‡
                patch_features = patch_features[:, 1:, :]  # å»æ‰ç¬¬ä¸€ä¸ªSEP token
            
            return patch_features
        elif isinstance(model, PapageiModel):
            # Papageiæ¨¡å‹
            if model == self.teacher_model:
                with torch.no_grad():
                    # ä½¿ç”¨Papageiæ¨¡å‹çš„patchç‰¹å¾æå–æ–¹æ³•
                    patch_features = model.extract_patch_features(x)  # [B, n_patches, dim]
            else:
                patch_features = model.extract_patch_features(x)  # [B, n_patches, dim]
            
            return patch_features
        else:
            return None
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.student_model.train()
        total_losses = {
            'total': 0, 'gt': 0, 'pred_distill': 0, 'feature_distill': 0, 'patch_feature_distill': 0, 'patch_distance_distill': 0
        }
        num_batches = 0
        
        # æ—¶é—´ç»Ÿè®¡
        batch_times = []
        epoch_start_time = time.time()
        
        progress_bar = tqdm.tqdm(self.train_loader, desc=f'è’¸é¦è®­ç»ƒ Epoch {epoch+1}')
        
        for batch_idx, data in enumerate(progress_bar):
            batch_start_time = time.time()
            
            ppg_segments = data["ppg_segments"].to(self.device)
            targets = None
            if "ft_label" in data:
                if self.task_type == 'regression':
                    targets = data["ft_label"].to(self.device).float()
                else:
                    targets = data["ft_label"].to(self.device).long()
            
            # Teacherå‰å‘ä¼ æ’­ï¼ˆæ— æ¢¯åº¦ï¼‰
            with torch.no_grad():
                teacher_output = self.teacher_model(ppg_segments)
                teacher_features = self.get_features(self.teacher_model, ppg_segments)
                # è·å–patchçº§åˆ«ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                teacher_patch_features = None
                if self.distill_loss.use_patch_feature_distill:
                    teacher_patch_features = self.get_patch_features(self.teacher_model, ppg_segments)
            
            # Studentå‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            student_output = self.student_model(ppg_segments)
            student_features = self.get_features(self.student_model, ppg_segments)
            # è·å–patchçº§åˆ«ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            student_patch_features = None
            if self.distill_loss.use_patch_feature_distill:
                student_patch_features = self.get_patch_features(self.student_model, ppg_segments)
            
            # è®¡ç®—è’¸é¦æŸå¤±
            loss_dict = self.distill_loss(
                student_output, teacher_output, student_features, 
                teacher_features, targets, self.task_type,
                student_patch_features, teacher_patch_features
            )
            
            # åå‘ä¼ æ’­
            loss_dict['total_loss'].backward()
            self.optimizer.step()
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆæ¯ä¸ªbatchè°ƒç”¨ï¼‰
            if self.scheduler:
                self.scheduler.step()
            
            # ç»Ÿè®¡
            for key in total_losses:
                if key == 'total':
                    total_losses[key] += loss_dict['total_loss'].item()
                elif key == 'gt':
                    total_losses[key] += loss_dict['gt_loss'].item()
                elif key == 'pred_distill':
                    total_losses[key] += loss_dict['pred_distill_loss'].item()
                elif key == 'feature_distill':
                    total_losses[key] += loss_dict['feature_distill_loss'].item()
                elif key == 'patch_feature_distill':
                    total_losses[key] += loss_dict['patch_feature_distill_loss'].item()
                elif key == 'patch_distance_distill':
                    total_losses[key] += loss_dict['patch_distance_distill_loss'].item()
            
            num_batches += 1
            
            # è®°å½•batchæ—¶é—´
            batch_time = time.time() - batch_start_time
            batch_times.append(batch_time)
            self.all_train_batch_times.append(batch_time)
            
            # æ›´æ–°è¿›åº¦æ¡ï¼ˆé™ä½æ›´æ–°é¢‘ç‡ï¼‰
            if batch_idx % 100 == 0 or batch_idx == len(self.train_loader) - 1:  # æ¯10ä¸ªbatchæˆ–æœ€åä¸€ä¸ªbatchæ›´æ–°ä¸€æ¬¡
                current_lr = self.scheduler.get_last_lr()[0] if self.scheduler else float(self.config['train_config'].get('lr_max', 30e-5))
                progress_bar.set_postfix({
                    'Total': f'{loss_dict["total_loss"].item():.4f}',
                    'GT': f'{loss_dict["gt_loss"].item():.4f}',
                    'Pred': f'{loss_dict["pred_distill_loss"].item():.4f}',
                    'Feat': f'{loss_dict["feature_distill_loss"].item():.4f}',
                    'LR': f'{current_lr:.2e}',
                    'Batch/s': f'{1.0/batch_time:.2f}'
                })
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_losses = {key: total_losses[key] / num_batches for key in total_losses}
        
        # è®¡ç®—æ—¶é—´ç»Ÿè®¡
        epoch_time = time.time() - epoch_start_time
        avg_batch_time = np.mean(batch_times)
        batches_per_second = 1.0 / avg_batch_time
        
        lprint(f"è’¸é¦è®­ç»ƒç»Ÿè®¡ - æ€»æ—¶é—´: {epoch_time:.2f}s, å¹³å‡æ¯æ‰¹: {avg_batch_time:.5f}s, æ‰¹æ¬¡/ç§’: {batches_per_second:.2f}")
        
        return avg_losses
    
    def validate(self):
        """éªŒè¯æ¨¡å‹"""
        self.student_model.eval()
        total_loss = 0
        num_batches = 0
        all_preds = []
        all_labels = []
        
        # æ—¶é—´ç»Ÿè®¡
        batch_times = []
        epoch_start_time = time.time()
        
        with torch.no_grad():
            for data in tqdm.tqdm(self.val_loader, desc='éªŒè¯'):
                batch_start_time = time.time()
                
                ppg_segments = data["ppg_segments"].to(self.device)
                
                if "ft_label" in data:
                    if self.task_type == 'regression':
                        labels = data["ft_label"].to(self.device).float()
                    else:
                        labels = data["ft_label"].to(self.device).long()
                    
                    # åªéœ€è¦studentè¾“å‡ºè¿›è¡ŒéªŒè¯
                    student_output = self.student_model(ppg_segments)
                    
                    if self.task_type == 'regression':
                        loss = F.mse_loss(student_output.squeeze(), labels)
                        all_preds.extend(student_output.squeeze().cpu().numpy())
                        all_labels.extend(labels.cpu().numpy())
                    else:  # classification
                        loss = F.cross_entropy(student_output, labels)
                        all_preds.extend(student_output.cpu().numpy())
                        all_labels.extend(labels.cpu().numpy())
                    
                    total_loss += loss.item()
                    num_batches += 1
                    
                    # è®°å½•batchæ—¶é—´
                    batch_time = time.time() - batch_start_time
                    batch_times.append(batch_time)
                    self.all_val_batch_times.append(batch_time)
        
        avg_val_loss = total_loss / num_batches if num_batches > 0 else 0
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)
        
        # è®¡ç®—æ—¶é—´ç»Ÿè®¡
        if batch_times:
            epoch_time = time.time() - epoch_start_time
            avg_batch_time = np.mean(batch_times)
            batches_per_second = 1.0 / avg_batch_time
            lprint(f"éªŒè¯ç»Ÿè®¡ - æ€»æ—¶é—´: {epoch_time:.2f}s, å¹³å‡æ¯æ‰¹: {avg_batch_time:.5f}s, æ‰¹æ¬¡/ç§’: {batches_per_second:.2f}")
        
        if self.task_type == 'regression':
            mse = mean_squared_error(all_labels, all_preds)
            mae = mean_absolute_error(all_labels, all_preds)
            return avg_val_loss, {'mse': mse, 'mae': mae}
        else:  # classification
            from sklearn.metrics import accuracy_score, f1_score
            pred_classes = np.argmax(all_preds, axis=1)
            acc = accuracy_score(all_labels, pred_classes)
            f1 = f1_score(all_labels, pred_classes, average='binary')
            return avg_val_loss, {'accuracy': acc, 'f1': f1}
    
    def test(self):
        """æµ‹è¯•æ¨¡å‹"""
        if self.test_loader is None:
            lprint("è­¦å‘Š: æ²¡æœ‰æµ‹è¯•æ•°æ®")
            return None, {}
            
        self.student_model.eval()
        total_loss = 0
        num_batches = 0
        all_preds = []
        all_labels = []
        
        # æ—¶é—´ç»Ÿè®¡
        batch_times = []
        epoch_start_time = time.time()
        
        with torch.no_grad():
            for data in tqdm.tqdm(self.test_loader, desc='æµ‹è¯•'):
                batch_start_time = time.time()
                
                ppg_segments = data["ppg_segments"].to(self.device)
                
                if "ft_label" in data:
                    if self.task_type == 'regression':
                        labels = data["ft_label"].to(self.device).float()
                    else:
                        labels = data["ft_label"].to(self.device).long()
                    
                    # åªéœ€è¦studentè¾“å‡ºè¿›è¡Œæµ‹è¯•
                    student_output = self.student_model(ppg_segments)
                    
                    if self.task_type == 'regression':
                        loss = F.mse_loss(student_output.squeeze(), labels)
                        all_preds.extend(student_output.squeeze().cpu().numpy())
                        all_labels.extend(labels.cpu().numpy())
                    else:  # classification
                        loss = F.cross_entropy(student_output, labels)
                        all_preds.extend(student_output.cpu().numpy())
                        all_labels.extend(labels.cpu().numpy())
                    
                    total_loss += loss.item()
                    num_batches += 1
                    
                    # è®°å½•batchæ—¶é—´
                    batch_time = time.time() - batch_start_time
                    batch_times.append(batch_time)
                    self.all_test_batch_times.append(batch_time)
        
        avg_test_loss = total_loss / num_batches if num_batches > 0 else 0
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)
        
        # è®¡ç®—æ—¶é—´ç»Ÿè®¡
        if batch_times:
            epoch_time = time.time() - epoch_start_time
            avg_batch_time = np.mean(batch_times)
            batches_per_second = 1.0 / avg_batch_time
            lprint(f"æµ‹è¯•ç»Ÿè®¡ - æ€»æ—¶é—´: {epoch_time:.2f}s, å¹³å‡æ¯æ‰¹: {avg_batch_time:.5f}s, æ‰¹æ¬¡/ç§’: {batches_per_second:.2f}")
        
        if self.task_type == 'regression':
            mse = mean_squared_error(all_labels, all_preds)
            mae = mean_absolute_error(all_labels, all_preds)
            return avg_test_loss, {'mse': mse, 'mae': mae}
        else:  # classification
            from sklearn.metrics import accuracy_score, f1_score
            pred_classes = np.argmax(all_preds, axis=1)
            acc = accuracy_score(all_labels, pred_classes)
            f1 = f1_score(all_labels, pred_classes, average='binary')
            return avg_test_loss, {'accuracy': acc, 'f1': f1}
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'student_model_state_dict': self.student_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_loss': self.best_val_loss,
            'best_epoch': self.best_epoch,
            'config': self.config
        }
        
        # ä¿å­˜å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
        if self.scheduler is not None:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        # å¦‚æœæœ‰ç‰¹å¾é€‚é…å™¨ï¼Œä¿å­˜å®ƒï¼ˆå…¨å±€å’Œpatchå…±ç”¨åŒä¸€ä¸ªï¼‰
        if self.distill_loss.feature_adapter is not None:
            checkpoint['feature_adapter_state_dict'] = self.distill_loss.feature_adapter.state_dict()
        
        # åªä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            torch.save(checkpoint, f"{self.save_path}_best.pth")
            lprint(f"ä¿å­˜æœ€ä½³æ¨¡å‹: {self.save_path}_best.pth")
    
    def save_training_log(self, log_data):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—åˆ°JSONæ–‡ä»¶"""
        log_file = f"{self.save_path}_distill_training_log.json"
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
        lprint(f"è’¸é¦è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {log_file}")
    
    def save_metrics_csv(self, metrics_history):
        """ä¿å­˜è®­ç»ƒæŒ‡æ ‡åˆ°CSVæ–‡ä»¶"""
        csv_file = f"{self.save_path}_distill_metrics.csv"
        
        if not metrics_history:
            return
        
        # è·å–æ‰€æœ‰å¯èƒ½çš„é”®
        all_keys = set()
        for epoch_data in metrics_history:
            all_keys.update(epoch_data.keys())
        
        # å†™å…¥CSV
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=sorted(all_keys))
            writer.writeheader()
            writer.writerows(metrics_history)
        
        lprint(f"è’¸é¦è®­ç»ƒæŒ‡æ ‡CSVå·²ä¿å­˜: {csv_file}")
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        epochs = self.config['train_config']['epochs']
        save_freq = self.config['train_config'].get('save_freq', 10)
        
        # åˆå§‹åŒ–æ—¥å¿—è®°å½•
        training_start_time = datetime.now()
        metrics_history = []
        
        # è·å–teacherå’Œstudentæ¨¡å‹å‚æ•°æ•°é‡
        teacher_total_params = sum(p.numel() for p in self.teacher_model.parameters())
        teacher_trainable_params = sum(p.numel() for p in self.teacher_model.parameters() if p.requires_grad)
        student_total_params = sum(p.numel() for p in self.student_model.parameters())
        student_trainable_params = sum(p.numel() for p in self.student_model.parameters() if p.requires_grad)
        
        training_log = {
            'experiment_info': {
                'experiment_type': 'knowledge_distillation',
                'teacher_type': 'inferred_from_path',  # å°†åœ¨mainå‡½æ•°ä¸­æ›´æ–°
                'student_type': 'inferred_from_config',  # å°†åœ¨mainå‡½æ•°ä¸­æ›´æ–°
                'dataset': 'inferred_from_config',  # å°†åœ¨mainå‡½æ•°ä¸­æ›´æ–°
                'task_type': self.task_type,
                'start_time': training_start_time.isoformat(),
                'config': self.config,
                'teacher_params': {
                    'total': teacher_total_params,
                    'trainable': teacher_trainable_params
                },
                'student_params': {
                    'total': student_total_params,
                    'trainable': student_trainable_params
                },
                'compression_ratio': teacher_total_params / student_total_params if student_total_params > 0 else 0
            },
            'training_history': [],
            'best_model_info': {}
        }
        
        lprint("å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ...")
        
        for epoch in range(epochs):
            epoch_start_time = time.time()
            lprint(f"\nEpoch {epoch+1}/{epochs}")
            
            # è®­ç»ƒ
            train_losses = self.train_epoch(epoch)
            # æ˜¾ç¤ºå½“å‰å­¦ä¹ ç‡
            current_lr = self.scheduler.get_last_lr()[0] if self.scheduler else float(self.config['train_config'].get('lr_max', 30e-5))
            lprint(f"å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
            
            lprint(f"è®­ç»ƒæŸå¤± - æ€»è®¡: {train_losses['total']:.4f}, "
                   f"GT: {train_losses['gt']:.4f}, "
                   f"é¢„æµ‹è’¸é¦: {train_losses['pred_distill']:.4f}, "
                   f"å…¨å±€ç‰¹å¾è’¸é¦: {train_losses['feature_distill']:.4f}, "
                   f"patchç‰¹å¾è’¸é¦: {train_losses['patch_feature_distill']:.4f}, "
                   f"patchè·ç¦»è’¸é¦: {train_losses['patch_distance_distill']:.4f}")
            
            # éªŒè¯
            val_loss = None
            val_metrics = {}
            if self.val_loader:
                val_loss, val_metrics = self.validate()
                lprint(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
                lprint(f"éªŒè¯æŒ‡æ ‡: {val_metrics}")
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
                is_best = val_loss < (self.best_val_loss - self.early_stop_min_delta)
                if is_best:
                    self.best_val_loss = val_loss
                    self.best_epoch = epoch
                    self.early_stop_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨
                    lprint(f"æ–°çš„æœ€ä½³æ¨¡å‹! éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
                else:
                    self.early_stop_counter += 1
                    lprint(f"éªŒè¯æŸå¤±æœªæ”¹å–„ ({self.early_stop_counter}/{self.early_stop_patience})")
            else:
                val_loss = train_losses['total']
                is_best = val_loss < (self.best_val_loss - self.early_stop_min_delta)
                if is_best:
                    self.best_val_loss = val_loss
                    self.best_epoch = epoch
                    self.early_stop_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨
                else:
                    self.early_stop_counter += 1
                    lprint(f"è®­ç»ƒæŸå¤±æœªæ”¹å–„ ({self.early_stop_counter}/{self.early_stop_patience})")
            
            # è®°å½•å½“å‰epochçš„æŒ‡æ ‡
            epoch_time = time.time() - epoch_start_time
            current_lr = self.scheduler.get_last_lr()[0] if self.scheduler else float(self.config['train_config'].get('lr_max', 30e-5))
            
            epoch_metrics = {
                'epoch': epoch + 1,
                'total_loss': train_losses['total'],
                'gt_loss': train_losses['gt'],
                'pred_distill_loss': train_losses['pred_distill'],
                'feature_distill_loss': train_losses['feature_distill'],
                'patch_feature_distill_loss': train_losses['patch_feature_distill'],
                'patch_distance_distill_loss': train_losses['patch_distance_distill'],
                'val_loss': val_loss,
                'learning_rate': current_lr,
                'epoch_time': epoch_time,
                'is_best': is_best,
                'early_stop_counter': self.early_stop_counter
            }
            
            # æ·»åŠ éªŒè¯æŒ‡æ ‡
            for key, value in val_metrics.items():
                epoch_metrics[f'val_{key}'] = value
            
            metrics_history.append(epoch_metrics)
            
            # è®°å½•åˆ°è®­ç»ƒæ—¥å¿—
            epoch_log = {
                'epoch': epoch + 1,
                'train_losses': train_losses,
                'val_loss': val_loss,
                'val_metrics': val_metrics,
                'learning_rate': current_lr,
                'epoch_time': epoch_time,
                'is_best': is_best,
                'timestamp': datetime.now().isoformat()
            }
            training_log['training_history'].append(epoch_log)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåªä¿å­˜æœ€ä½³æ¨¡å‹ï¼‰
            if is_best:
                self.save_checkpoint(epoch, is_best)
                
                # æ›´æ–°æœ€ä½³æ¨¡å‹ä¿¡æ¯
                training_log['best_model_info'] = {
                    'epoch': epoch + 1,
                    'val_loss': self.best_val_loss,
                    'train_losses': train_losses,
                    'val_metrics': val_metrics,
                    'model_path': f"{self.save_path}_best.pth"
                }
            
            # æ—©åœæ£€æŸ¥
            if self.early_stop_counter >= self.early_stop_patience:
                self.early_stopped = True
                lprint(f"ğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­ {self.early_stop_patience} ä¸ªepochæ— æ˜¾è‘—æ”¹å–„")
                lprint(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f} (Epoch {self.best_epoch + 1})")
                break
        
        # è®­ç»ƒç»“æŸï¼Œæ›´æ–°æ—¥å¿—
        training_end_time = datetime.now()
        training_log['experiment_info']['end_time'] = training_end_time.isoformat()
        training_log['experiment_info']['total_training_time'] = str(training_end_time - training_start_time)
        training_log['experiment_info']['best_epoch'] = self.best_epoch + 1
        training_log['experiment_info']['best_val_loss'] = self.best_val_loss
        training_log['experiment_info']['early_stopped'] = self.early_stopped
        training_log['experiment_info']['early_stop_patience'] = self.early_stop_patience
        training_log['experiment_info']['early_stop_min_delta'] = self.early_stop_min_delta
        
        # è®¡ç®—æ€§èƒ½ç»Ÿè®¡
        performance_stats = {}
        if self.all_train_batch_times:
            avg_train_batch_time = np.mean(self.all_train_batch_times)
            performance_stats['avg_train_batch_time_seconds'] = avg_train_batch_time
            performance_stats['avg_train_batches_per_second'] = 1.0 / avg_train_batch_time
            performance_stats['total_train_batches'] = len(self.all_train_batch_times)
        
        if self.all_val_batch_times:
            avg_val_batch_time = np.mean(self.all_val_batch_times)
            performance_stats['avg_val_batch_time_seconds'] = avg_val_batch_time
            performance_stats['avg_val_batches_per_second'] = 1.0 / avg_val_batch_time
            performance_stats['total_val_batches'] = len(self.all_val_batch_times)
        
        training_log['performance_stats'] = performance_stats
        
        if self.early_stopped:
            lprint(f"\nè®­ç»ƒæå‰åœæ­¢! æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f} (Epoch {self.best_epoch+1})")
            lprint(f"æ—©åœåŸå› : è¿ç»­ {self.early_stop_patience} ä¸ªepochéªŒè¯æŸå¤±æ— æ˜¾è‘—æ”¹å–„ (é˜ˆå€¼: {self.early_stop_min_delta})")
        else:
            lprint(f"\nè®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f} (Epoch {self.best_epoch+1})")
        
        # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
        if performance_stats:
            lprint("\nè’¸é¦è®­ç»ƒæ€§èƒ½ç»Ÿè®¡:")
            if 'avg_train_batches_per_second' in performance_stats:
                lprint(f"å¹³å‡è®­ç»ƒæ‰¹æ¬¡/ç§’: {performance_stats['avg_train_batches_per_second']:.2f} "
                       f"(åŸºäº {performance_stats['total_train_batches']} ä¸ªæ‰¹æ¬¡)")
            if 'avg_val_batches_per_second' in performance_stats:
                lprint(f"å¹³å‡éªŒè¯æ‰¹æ¬¡/ç§’: {performance_stats['avg_val_batches_per_second']:.2f} "
                       f"(åŸºäº {performance_stats['total_val_batches']} ä¸ªæ‰¹æ¬¡)")
        
        # ä¿å­˜è®­ç»ƒæ—¥å¿—å’ŒæŒ‡æ ‡CSV
        self.save_training_log(training_log)
        self.save_metrics_csv(metrics_history)
        
        # è®­ç»ƒå®Œæˆåçš„æµ‹è¯•éœ€è¦åŠ è½½æœ€ä¼˜checkpoint
        if self.test_loader:
            # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
            checkpoint_path = f"{self.save_path}_best.pth"
            if os.path.exists(checkpoint_path):
                lprint(f"\nåŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•: {checkpoint_path}")
                checkpoint = torch.load(checkpoint_path, map_location=self.device)
                self.student_model.load_state_dict(checkpoint['student_model_state_dict'])
                
                # å¦‚æœæœ‰å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€ï¼ŒåŠ è½½å®ƒ
                if self.scheduler is not None and 'scheduler_state_dict' in checkpoint:
                    self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                    lprint("å·²åŠ è½½å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€")
                
                # å¦‚æœæœ‰ç‰¹å¾é€‚é…å™¨ï¼ŒåŠ è½½å®ƒï¼ˆå…¨å±€å’Œpatchå…±ç”¨åŒä¸€ä¸ªï¼‰
                if self.distill_loss.feature_adapter is not None and 'feature_adapter_state_dict' in checkpoint:
                    self.distill_loss.feature_adapter.load_state_dict(checkpoint['feature_adapter_state_dict'])
                    lprint("å·²åŠ è½½ç‰¹å¾é€‚é…å™¨")
                
                test_loss, test_metrics = self.test()
                
                # è®¡ç®—æµ‹è¯•æ€§èƒ½ç»Ÿè®¡
                test_performance = {}
                if self.all_test_batch_times:
                    avg_test_batch_time = np.mean(self.all_test_batch_times)
                    test_performance['avg_test_batch_time_seconds'] = avg_test_batch_time
                    test_performance['avg_test_batches_per_second'] = 1.0 / avg_test_batch_time
                    test_performance['total_test_batches'] = len(self.all_test_batch_times)
                    
                    lprint(f"è’¸é¦æµ‹è¯•æ€§èƒ½ç»Ÿè®¡:")
                    lprint(f"å¹³å‡æµ‹è¯•æ‰¹æ¬¡/ç§’: {test_performance['avg_test_batches_per_second']:.2f} "
                           f"(åŸºäº {test_performance['total_test_batches']} ä¸ªæ‰¹æ¬¡)")
                
                # å°†æµ‹è¯•ç»“æœæ·»åŠ åˆ°æ—¥å¿—
                training_log['test_results'] = {
                    'test_loss': test_loss,
                    'test_metrics': test_metrics,
                    'test_time': datetime.now().isoformat(),
                    'test_performance': test_performance
                }
                
                # é‡æ–°ä¿å­˜åŒ…å«æµ‹è¯•ç»“æœçš„æ—¥å¿—
                self.save_training_log(training_log)
                
                return test_loss, test_metrics
            else:
                lprint(f"è­¦å‘Š: æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æ–‡ä»¶ {checkpoint_path}ï¼Œè·³è¿‡æµ‹è¯•")
                training_log['test_results'] = {
                    'error': 'Best model file not found',
                    'test_time': datetime.now().isoformat()
                }
                self.save_training_log(training_log)
                return None, {}
        else:
            training_log['test_results'] = {
                'warning': 'No test data provided',
                'test_time': datetime.now().isoformat()
            }
            self.save_training_log(training_log)
            return None, {}


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆæ”¯æŒYAMLå’ŒJSONï¼‰"""
    assert os.path.isfile(config_path), f'{config_path=}'
    
    if config_path.endswith('.yaml') or config_path.endswith('.yml'):
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
    elif config_path.endswith('.json'):
        with open(config_path, 'r') as f:
            config = json.load(f)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®æ–‡ä»¶æ ¼å¼: {config_path}")
    
    lprint(f"é…ç½®æ–‡ä»¶å·²åŠ è½½: {config_path}")
    lprint(json.dumps(config, indent=2))
    return config


def auto_load_distill_configs(teacher_type, student_type, dataset):
    """æ ¹æ®teacherã€studentç±»å‹å’Œæ•°æ®é›†è‡ªåŠ¨åŠ è½½è’¸é¦é…ç½®æ–‡ä»¶"""
    # æ¨æ–­è’¸é¦é…ç½®æ–‡ä»¶è·¯å¾„
    if teacher_type == 'papagei' and student_type == 'mlp':
        distill_config_path = "config/distillation/papagei_to_mlp_distill.yaml"
    elif teacher_type == 'papagei' and student_type == 'linear':
        distill_config_path = "config/distillation/papagei_to_linear_distill.yaml"
    elif teacher_type == 'papagei' and student_type.startswith('gpt'):
        # Papageiåˆ°GPTçš„è’¸é¦ï¼Œæ”¯æŒpatchçº§åˆ«ç‰¹å¾è’¸é¦
        distill_config_path = "config/distillation/papagei_to_gpt_distill.yaml"
    elif teacher_type.startswith('gpt') and student_type.startswith('gpt'):
        # GPTåˆ°GPTçš„è’¸é¦ï¼Œæ”¯æŒpatchçº§åˆ«ç‰¹å¾è’¸é¦
        distill_config_path = "config/distillation/gpt_to_gpt_patch_distill.yaml"
        # å¦‚æœä¸å­˜åœ¨patché…ç½®ï¼Œå›é€€åˆ°æ™®é€šé…ç½®
        if not os.path.exists(distill_config_path):
            distill_config_path = "config/distillation/gpt_to_mlp_distill.yaml"
            lprint(f"è­¦å‘Š: æœªæ‰¾åˆ°GPTåˆ°GPTçš„patchè’¸é¦é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    elif teacher_type.startswith('gpt') and student_type == 'mlp':
        distill_config_path = "config/distillation/gpt_to_mlp_distill.yaml"
    elif teacher_type.startswith('gpt') and student_type == 'linear':
        distill_config_path = "config/distillation/gpt_to_linear_distill.yaml"
    elif teacher_type == 'mlp' and student_type == 'linear':
        distill_config_path = "config/distillation/mlp_to_linear_distill.yaml"
    else:
        # é»˜è®¤ä½¿ç”¨gpt_to_mlpé…ç½®
        distill_config_path = "config/distillation/gpt_to_mlp_distill.yaml"
    
    # æ¨æ–­å­¦ç”Ÿæ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
    if student_type in ['gpt_19m', 'gpt_1m']:
        student_config_path = f"config/models/gpt_config_{dataset}.yaml"
    else:
        student_config_path = f"config/models/{student_type}_config_{dataset}.yaml"
    
    # æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
    data_config_path = f"config/data/{dataset}_data.yaml"
    
    # åŠ è½½é…ç½®
    distill_config = load_config(distill_config_path)
    student_model_config = load_config(student_config_path)
    data_config = load_config(data_config_path)
    
    # åˆå¹¶é…ç½®
    config = {
        'distillation': distill_config['distillation'],
        'train_config': distill_config['train_config'],
        'data_config': data_config['data_config']
    }
    
    # è·å–ä»»åŠ¡ç±»å‹
    task_type = data_config['data_config'].get('task_type', 'regression')
    
    return config, student_model_config['model_config'], task_type


def auto_infer_teacher_path(teacher_type, dataset, save_dir):
    """è‡ªåŠ¨æ¨æ–­teacheræ¨¡å‹è·¯å¾„"""
    teacher_path = os.path.join(save_dir, f"{teacher_type}_{dataset}_best.pth")
    return teacher_path


def create_model(model_config, model_type='gpt_19m'):
    """åˆ›å»ºæŒ‡å®šç±»å‹çš„æ¨¡å‹"""
    if model_type.lower() == 'papagei':
        try:
            model = create_papagei_model(model_config)
        except Exception as e:
            lprint(f"åˆ›å»ºPapageiæ¨¡å‹å¤±è´¥: {e}")
            model = PapageiModel(model_config)
    elif model_type.lower() in ['gpt', 'gpt_19m', 'gpt_1m']:
        # å¦‚æœæ˜¯GPTæ¨¡å‹ï¼Œæ ¹æ®ç±»å‹åŠ è½½å¯¹åº”é…ç½®
        if model_type.lower() == 'gpt_1m':
            # åŠ è½½GPT-1Mé…ç½®
            gpt_1m_config_path = 'config/gpt_1M.json'
            if os.path.exists(gpt_1m_config_path):
                gpt_1m_config = load_config(gpt_1m_config_path)
                # åˆå¹¶é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨GPT-1Mçš„æ¶æ„å‚æ•°
                merged_config = model_config.copy()
                merged_config.update(gpt_1m_config)
            else:
                # ä½¿ç”¨é»˜è®¤1Mé…ç½®
                gpt_1m_config = {
                    'd_model': 128,
                    'n_heads': 4,
                    'n_layers': 5,
                    'patch_size': 40,
                    'dropout': 0.1,
                    'max_len': 2400,
                    'PARAMS': '1M'
                }
                merged_config = model_config.copy()
                merged_config.update(gpt_1m_config)
            
            # æ·»åŠ ç¼ºå¤±çš„å¿…éœ€å­—æ®µ
            required_fields = {
                'tune_mode': 'full',
                'use_penultimate_layer': False,
                'is_input_multichannel': False,
                'ecg_input': False,
                'fuse_tuning': False,
                'fuse_feat_type': None,
                'use_lora': False,
                'out_dim_override': None,
                'pooling_fxn': 'linear'
            }
            
            for key, default_value in required_fields.items():
                if key not in merged_config:
                    merged_config[key] = default_value
            
            # ç¡®ä¿PARAMSå­—æ®µæ­£ç¡®è®¾ç½®
            merged_config['PARAMS'] = '1M'
            model = GPT_with_linearOutput(merged_config)
        else:
            # GPT-19Mæˆ–é»˜è®¤GPT
            model = GPT_with_linearOutput(model_config)
    elif model_type.lower() == 'linear':
        try:
            model = create_linear_model(model_config)
        except:
            model = LinearModel(model_config)
    elif model_type.lower() == 'mlp':
        try:
            model = create_mlp_model(model_config)
        except:
            # è®¡ç®—è¾“å…¥å¤§å°
            patch_size = model_config.get('patch_size', 40)
            n_patches = model_config.get('n_patches', 30)
            input_size = patch_size * n_patches
            
            mlp_config = {
                'input_size': input_size,
                'hidden_sizes': model_config.get('hidden_sizes', [512, 256, 128]),
                'output_size': model_config.get('output_size', 1),
                'dropout': model_config.get('dropout', 0.2),
                'activation': model_config.get('activation', 'relu'),
                'batch_norm': model_config.get('batch_norm', True)
            }
            model = MLP(**mlp_config)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    lprint(f"åˆ›å»º{model_type.upper()}æ¨¡å‹:")
    lprint(model)
    return model


def load_teacher_model(teacher_path, teacher_type, device, n_patches=None):
    """åŠ è½½é¢„è®­ç»ƒçš„teacheræ¨¡å‹"""
    lprint(f"åŠ è½½teacheræ¨¡å‹: {teacher_path} (ç±»å‹: {teacher_type})")
    
    checkpoint = torch.load(teacher_path, map_location='cpu')
    
    # æ¨æ–­æ•°æ®é›†ï¼ˆä»teacherè·¯å¾„ï¼‰
    if 'dalia' in teacher_path:
        dataset = 'dalia'
    elif 'stanfordAF' in teacher_path:
        dataset = 'stanfordAF'
    else:
        dataset = 'dalia'  # é»˜è®¤ä½¿ç”¨dalia
    
    # åŠ è½½å®Œæ•´çš„æ¨¡å‹é…ç½®
    if teacher_type in ['gpt_19m', 'gpt_1m']:
        model_config_path = f"config/models/gpt_config_{dataset}.yaml"
    else:
        model_config_path = f"config/models/{teacher_type}_config_{dataset}.yaml"
    
    if os.path.exists(model_config_path):
        full_config = load_config(model_config_path)
        teacher_config = full_config['model_config'].copy()
        
        # æ·»åŠ ç¼ºå¤±çš„å¿…éœ€å­—æ®µï¼ˆä»…å¯¹GPTæ¨¡å‹ï¼‰
        if teacher_type in ['gpt_19m', 'gpt_1m']:
            required_fields = {
                'gpt_state_dict_path': None,
                'strict_loading_gpt_state_dict': True,
                'tune_mode': 'full',
                'use_penultimate_layer': False,
                'is_input_multichannel': False,
                'ecg_input': False,
                'fuse_tuning': False,
                'fuse_feat_type': None,
                'use_lora': False,
                'out_dim_override': 2,
                'pooling_fxn': 'linear'
            }
            
            for key, default_value in required_fields.items():
                if key not in teacher_config:
                    teacher_config[key] = default_value
        
        # å¦‚æœæä¾›äº†n_patchesï¼Œä½¿ç”¨è®¡ç®—å¾—åˆ°çš„å€¼
        if n_patches is not None:
            teacher_config['n_patches'] = n_patches
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´æ¶æ„å‚æ•°
        if teacher_type == 'gpt_1m':
            # åŠ è½½GPT-1Mçš„æ¶æ„å‚æ•°
            gpt_1m_config_path = 'config/gpt_1M.json'
            if os.path.exists(gpt_1m_config_path):
                gpt_1m_config = load_config(gpt_1m_config_path)
                # æ›´æ–°æ¶æ„ç›¸å…³å‚æ•°
                for key in ['d_model', 'n_heads', 'n_layers', 'dropout', 'max_len']:
                    if key in gpt_1m_config:
                        teacher_config[key] = gpt_1m_config[key]
                teacher_config['PARAMS'] = '1M'
        elif teacher_type == 'gpt_19m':
            # ç¡®ä¿ä½¿ç”¨GPT-19Mçš„å‚æ•°
            teacher_config['PARAMS'] = '19M'
    else:
        # å¦‚æœæ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        lprint(f"è­¦å‘Š: æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ {model_config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        teacher_config = {
            'model_name': 'gpt',
            'PARAMS': '1M' if teacher_type == 'gpt_1m' else '19M',
            'patch_size': 40,
            'd_model': 128 if teacher_type == 'gpt_1m' else 512,
            'n_heads': 4 if teacher_type == 'gpt_1m' else 8,
            'n_layers': 5 if teacher_type == 'gpt_1m' else 6,
            'dropout': 0.1,
            'max_len': 2400,
            'loss': 'laplace',
            'with_conv': False,
            'output_size': 1,
            'out_classes': 1,
            'apply_mask': False,
            'n_patches': n_patches if n_patches is not None else 30,
            'gpt_state_dict_path': None,
            'strict_loading_gpt_state_dict': True,
            'tune_mode': 'full',
            'use_penultimate_layer': False,
            'is_input_multichannel': False,
            'ecg_input': False,
            'fuse_tuning': False,
            'fuse_feat_type': None,
            'use_lora': False,
            'out_dim_override': 2,
            'pooling_fxn': 'linear'
        }
    
    # åˆ›å»ºteacheræ¨¡å‹
    teacher_model = create_model(teacher_config, teacher_type)
    
    # åŠ è½½æƒé‡
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
    elif 'student_model_state_dict' in checkpoint:
        state_dict = checkpoint['student_model_state_dict']
    elif 'model' in checkpoint:
        # è¿™æ˜¯ä»train.pyä¿å­˜çš„checkpointæ ¼å¼
        state_dict = checkpoint['model']
    else:
        state_dict = checkpoint
    
    # ç§»é™¤torch.compileäº§ç”Ÿçš„_orig_mod.å‰ç¼€
    if any(key.startswith('_orig_mod.') for key in state_dict.keys()):
        new_state_dict = {}
        for key, value in state_dict.items():
            if key.startswith('_orig_mod.'):
                new_key = key[len('_orig_mod.'):]  # ç§»é™¤å‰ç¼€
                new_state_dict[new_key] = value
            else:
                new_state_dict[key] = value
        state_dict = new_state_dict
    
    teacher_model.load_state_dict(state_dict)
    
    teacher_model = teacher_model.to(device)
    teacher_model.eval()  # è®¾ä¸ºè¯„ä¼°æ¨¡å¼
    
    # å†»ç»“teacheræ¨¡å‹å‚æ•°
    for param in teacher_model.parameters():
        param.requires_grad = False
    
    lprint("Teacheræ¨¡å‹åŠ è½½å®Œæˆå¹¶å·²å†»ç»“")
    return teacher_model


def count_parameters(model):
    """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
    # å¦‚æœæ˜¯Papageiæ¨¡å‹ï¼Œä½¿ç”¨ä¸“é—¨çš„å‚æ•°è®¡ç®—å‡½æ•°
    if isinstance(model, PapageiModel):
        return count_papagei_parameters(model)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    lprint(f"æ€»å‚æ•°æ•°: {total_params:,}")
    lprint(f"å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")
    return total_params, trainable_params


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€PPGæ¨¡å‹çŸ¥è¯†è’¸é¦è®­ç»ƒ')
    parser.add_argument('--teacher_type', type=str, required=True,
                      choices=['gpt_19m', 'gpt_1m', 'linear', 'mlp', 'papagei'], help='Teacheræ¨¡å‹ç±»å‹')
    parser.add_argument('--student_type', type=str, required=True,
                      choices=['gpt_19m', 'gpt_1m', 'linear', 'mlp', 'papagei'], help='Studentæ¨¡å‹ç±»å‹')
    parser.add_argument('--dataset', type=str, required=True,
                      choices=['dalia', 'stanfordAF'], help='æ•°æ®é›†åç§°')
    parser.add_argument('--teacher_path', type=str, help='Teacheræ¨¡å‹è·¯å¾„ (å¯é€‰ï¼Œè‡ªåŠ¨æ¨æ–­)')
    parser.add_argument('--save_dir', type=str, default='./output', help='Teacheræ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--save_dir_student', type=str, default='./output_s', help='Studentæ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--test_only', action='store_true', help='åªè¿›è¡Œæµ‹è¯•')
    parser.add_argument('--no-test', action='store_true', help='ç¦ç”¨è®­ç»ƒå®Œæˆåçš„è‡ªåŠ¨æµ‹è¯•')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # è‡ªåŠ¨åŠ è½½é…ç½®æ–‡ä»¶
    config, student_config, task_type = auto_load_distill_configs(
        args.teacher_type, args.student_type, args.dataset
    )
    
    # è‡ªåŠ¨æ¨æ–­teacheræ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœæœªæä¾›ï¼‰
    if args.teacher_path is None:
        teacher_path = auto_infer_teacher_path(args.teacher_type, args.dataset, args.save_dir)
    else:
        teacher_path = args.teacher_path
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    lprint(f"ä½¿ç”¨è®¾å¤‡: {device}")
    lprint(f"æ•°æ®é›†: {args.dataset}, Teacher: {args.teacher_type}, Student: {args.student_type}, ä»»åŠ¡ç±»å‹: {task_type}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir_student, exist_ok=True)
    
    # æ„å»ºä¿å­˜è·¯å¾„
    save_path = os.path.join(args.save_dir_student, 
                           f"distill_{args.teacher_type}_to_{args.student_type}_{args.dataset}")
    
    # æ•°æ®åŠ è½½å’Œn_patchesè®¡ç®—
    data_cfg = config['data_config']
    lprint("åŠ è½½æ•°æ®...")
    
    # è·å–patch_size
    patch_size = student_config.get('patch_size', 40)
    
    # è®¡ç®—n_patches
    n_patches = calc_n_patches(data_cfg['train_data_path'], patch_size)
    lprint(f"è®¡ç®—å¾—åˆ°çš„patchæ•°é‡: {n_patches}")
    
    # æ›´æ–°student_configä¸­çš„n_patches
    student_config['n_patches'] = n_patches
    
    # åŠ è½½teacheræ¨¡å‹
    teacher_model = load_teacher_model(teacher_path, args.teacher_type, device, n_patches)
    lprint("Teacheræ¨¡å‹:")
    count_parameters(teacher_model)
    
    # åˆ›å»ºstudentæ¨¡å‹
    student_model = create_model(student_config, args.student_type)
    student_model = student_model.to(device)
    lprint("Studentæ¨¡å‹:")
    count_parameters(student_model)
    
    train_dataset = PretrainDataset(
        data_cfg['train_data_path'],
        patch_size=patch_size,
        train_labels_dataset_path=data_cfg.get('train_label_path', ''),
        data_red_factor=data_cfg.get('data_red_factor', 1)
    )
    
    val_dataset = None
    if data_cfg.get('val_data_path'):
        val_dataset = PretrainDataset(
            data_cfg['val_data_path'],
            patch_size=patch_size,
            train_labels_dataset_path=data_cfg.get('val_label_path', ''),
            data_red_factor=data_cfg.get('data_red_factor', 1)
        )
    
    # æµ‹è¯•æ•°æ®é›†
    test_dataset = None
    if data_cfg.get('test_data_path'):
        test_dataset = PretrainDataset(
            data_cfg['test_data_path'],
            patch_size=patch_size,
            train_labels_dataset_path=data_cfg.get('test_label_path', ''),
            data_red_factor=data_cfg.get('data_red_factor', 1)
        )
    
    train_config = config['train_config']
    train_loader = DataLoader(
        train_dataset,
        batch_size=train_config['batch_size'],
        shuffle=True,
        num_workers=0,
        pin_memory=False
    )
    
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=train_config['batch_size'],
            shuffle=False,
            num_workers=0,
            pin_memory=False
        )
    
    test_loader = None
    if test_dataset:
        test_loader = DataLoader(
            test_dataset,
            batch_size=train_config['batch_size'],
            shuffle=False,
            num_workers=0,
            pin_memory=False
        )
    
    # å¦‚æœåªæ˜¯æµ‹è¯•ï¼ŒåŠ è½½æ¨¡å‹å¹¶æµ‹è¯•
    if args.test_only:
        checkpoint_path = f"{save_path}_best.pth"
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path, map_location=device)
            student_model.load_state_dict(checkpoint['student_model_state_dict'])
            lprint(f"å·²åŠ è½½å­¦ç”Ÿæ¨¡å‹: {checkpoint_path}")
            
            if test_loader:
                trainer = DistillationTrainer(
                    teacher_model=teacher_model,
                    student_model=student_model,
                    train_loader=train_loader,
                    val_loader=val_loader,
                    config=config,
                    device=device,
                    save_path=save_path,
                    task_type=task_type,
                    test_loader=test_loader
                )
                
                # å¦‚æœæœ‰å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€ï¼ŒåŠ è½½å®ƒ
                if trainer.scheduler is not None and 'scheduler_state_dict' in checkpoint:
                    trainer.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                    lprint("å·²åŠ è½½å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€")
                
                # å¦‚æœæœ‰ç‰¹å¾é€‚é…å™¨ï¼ŒåŠ è½½å®ƒï¼ˆå…¨å±€å’Œpatchå…±ç”¨åŒä¸€ä¸ªï¼‰
                if trainer.distill_loss.feature_adapter is not None and 'feature_adapter_state_dict' in checkpoint:
                    trainer.distill_loss.feature_adapter.load_state_dict(checkpoint['feature_adapter_state_dict'])
                    lprint("å·²åŠ è½½ç‰¹å¾é€‚é…å™¨")
                
                test_loss, test_metrics = trainer.test()
                lprint(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}")
                lprint(f"æµ‹è¯•æŒ‡æ ‡: {test_metrics}")
            else:
                lprint("æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®")
        else:
            lprint(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {checkpoint_path}")
        return
    
    # åˆ›å»ºè’¸é¦è®­ç»ƒå™¨
    trainer = DistillationTrainer(
        teacher_model=teacher_model,
        student_model=student_model,
        train_loader=train_loader,
        val_loader=val_loader,
        config=config,
        device=device,
        save_path=save_path,
        task_type=task_type,
        test_loader=test_loader
    )
    
    # åœ¨è®­ç»ƒå¼€å§‹å‰æ›´æ–°trainerçš„æ—¥å¿—ä¿¡æ¯ï¼ˆé€šè¿‡monkey patchï¼‰
    def update_training_log_info(trainer, teacher_type, student_type, dataset):
        """æ›´æ–°è®­ç»ƒå™¨çš„æ—¥å¿—ä¿¡æ¯"""
        # è¿™ä¸ªæ–¹æ³•ä¼šåœ¨train()æ–¹æ³•ä¸­è¢«è°ƒç”¨æ¥æ›´æ–°experiment_info
        original_train = trainer.train
        
        def enhanced_train():
            result = original_train()
            # è®­ç»ƒå®Œæˆåï¼Œè¯»å–å¹¶æ›´æ–°æ—¥å¿—æ–‡ä»¶
            try:
                log_file = f"{trainer.save_path}_distill_training_log.json"
                if os.path.exists(log_file):
                    with open(log_file, 'r', encoding='utf-8') as f:
                        training_log = json.load(f)
                    
                    # æ›´æ–°å®éªŒä¿¡æ¯
                    training_log['experiment_info']['teacher_type'] = teacher_type
                    training_log['experiment_info']['student_type'] = student_type
                    training_log['experiment_info']['dataset'] = dataset
                    training_log['experiment_info']['teacher_path'] = teacher_path
                    
                    # é‡æ–°ä¿å­˜
                    with open(log_file, 'w', encoding='utf-8') as f:
                        json.dump(training_log, f, indent=2, ensure_ascii=False)
            except Exception as e:
                lprint(f"æ›´æ–°æ—¥å¿—ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            
            return result
        
        trainer.train = enhanced_train
    
    # åº”ç”¨æ—¥å¿—ä¿¡æ¯æ›´æ–°
    update_training_log_info(trainer, args.teacher_type, args.student_type, args.dataset)
    
    # å¼€å§‹è®­ç»ƒ
    test_loss, test_metrics = trainer.train()
    
    # å¦‚æœå¯ç”¨äº†æµ‹è¯•ä¸”æœ‰æµ‹è¯•æ•°æ®ï¼Œæ˜¾ç¤ºæœ€ç»ˆæµ‹è¯•ç»“æœ
    if not args.no_test and test_loader and test_loss is not None:
        lprint(f"\næœ€ç»ˆæµ‹è¯•æŸå¤±: {test_loss:.4f}")
        lprint(f"æœ€ç»ˆæµ‹è¯•æŒ‡æ ‡: {test_metrics}")
    elif not args.no_test and not test_loader:
        lprint("è­¦å‘Š: å¯ç”¨äº†è‡ªåŠ¨æµ‹è¯•ä½†æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®")


if __name__ == "__main__":
    main() 